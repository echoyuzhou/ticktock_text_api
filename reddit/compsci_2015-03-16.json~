[{"docID": "t5_2qhmr", "qSentId": 1, "question": "Big-O Misconceptions", "aSentId": 2, "answer": "Interesting, at my university, we primarily used theta. It really forced you to think harder about runtime. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 3, "question": "Interesting, at my university, we primarily used theta. It really forced you to think harder about runtime. ", "aSentId": 4, "answer": "The thing is, proving lower bounds is MUCH HARDER than proving upper bounds! Looking at comparison sorts, the `n log n` upper bound is much easier to derive than the `n log n` lower bound.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 5, "question": "The thing is, proving lower bounds is MUCH HARDER than proving upper bounds! Looking at comparison sorts, the `n log n` upper bound is much easier to derive than the `n log n` lower bound.", "aSentId": 6, "answer": "That's one of the reasons my algorithm class was unusually hard. Some algorithms wouldn't fit into theta, so we would have to prove both O and Omega. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 5, "question": "The thing is, proving lower bounds is MUCH HARDER than proving upper bounds! Looking at comparison sorts, the `n log n` upper bound is much easier to derive than the `n log n` lower bound.", "aSentId": 8, "answer": "The difficulty of those statements isn't fair to compare.  The upper bound is the worst case runtime of a specific algorithm.  So, once you have an algorithm like mergesort, we can see that it takes n lg n time.  The lower bound is a lower bound on the general problem of comparison based sorting.  For any comparison based sorting algorithm, it cannot have worst case runtime better than order n log n.\nFor a particular algorithm, proving it's upper and lower bounds might not be too hard from each other, but here, you are comparing the upper bound of one algorithm, vs. proving the non-existence of algorithms with worst-case runtimes of less than n log n.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 9, "question": "The difficulty of those statements isn't fair to compare.  The upper bound is the worst case runtime of a specific algorithm.  So, once you have an algorithm like mergesort, we can see that it takes n lg n time.  The lower bound is a lower bound on the general problem of comparison based sorting.  For any comparison based sorting algorithm, it cannot have worst case runtime better than order n log n.\nFor a particular algorithm, proving it's upper and lower bounds might not be too hard from each other, but here, you are comparing the upper bound of one algorithm, vs. proving the non-existence of algorithms with worst-case runtimes of less than n log n.", "aSentId": 10, "answer": "In both cases, I was actually referring to the general problem of comparison sorting.\n\nThe \"unfair\" issue here is exactly why it's harder to show lower bounds than upper bounds. To show an upper bound, you find an algorithm that solves your problem, and you find out its time complexity. To show a lower bound, you need to show that *there can be no algorithm solving this problem which takes less time than this.*\n\nThe easiest example I can think of is showing that searching an unsorted array takes at least linear time, because it has to look at every single element of the array in the worst case. Comparison sorts are a harder problem, but still tractable. Other problems, even simple ones, have no known lower bounds.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 11, "question": "In both cases, I was actually referring to the general problem of comparison sorting.\n\nThe \"unfair\" issue here is exactly why it's harder to show lower bounds than upper bounds. To show an upper bound, you find an algorithm that solves your problem, and you find out its time complexity. To show a lower bound, you need to show that *there can be no algorithm solving this problem which takes less time than this.*\n\nThe easiest example I can think of is showing that searching an unsorted array takes at least linear time, because it has to look at every single element of the array in the worst case. Comparison sorts are a harder problem, but still tractable. Other problems, even simple ones, have no known lower bounds.", "aSentId": 12, "answer": "What is the upper bound on general comparison-based sorting? I would assume it's O(n^(2)), but are there any correct sorting algorithms which are O(n^(3)) that don't use any unnecessary steps?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 13, "question": "What is the upper bound on general comparison-based sorting? I would assume it's O(n^(2)), but are there any correct sorting algorithms which are O(n^(3)) that don't use any unnecessary steps?", "aSentId": 14, "answer": "comparison sorting is tightly bounded by n log n, both above and below. n^2 is a looser upper bound.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 15, "question": "comparison sorting is tightly bounded by n log n, both above and below. n^2 is a looser upper bound.", "aSentId": 16, "answer": "What about quicksort? That has an upper bound of n^2 yet is a comparison based sorting algorithm.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 13, "question": "What is the upper bound on general comparison-based sorting? I would assume it's O(n^(2)), but are there any correct sorting algorithms which are O(n^(3)) that don't use any unnecessary steps?", "aSentId": 18, "answer": "You can always build a worse algorithm, so there is no upper bound for the problem.  Worst-case comparison based sorting takes Omega(n lg n).  There exist algorithms that solve the problem in O(n lg n).  By those two statements, we say that the PROBLEM is an n lg n problem.  (We can use asymptotic notation to talk about the complexity of an algorithm, and also a problem as a whole.  Here, we are going with the latter.)\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 11, "question": "In both cases, I was actually referring to the general problem of comparison sorting.\n\nThe \"unfair\" issue here is exactly why it's harder to show lower bounds than upper bounds. To show an upper bound, you find an algorithm that solves your problem, and you find out its time complexity. To show a lower bound, you need to show that *there can be no algorithm solving this problem which takes less time than this.*\n\nThe easiest example I can think of is showing that searching an unsorted array takes at least linear time, because it has to look at every single element of the array in the worst case. Comparison sorts are a harder problem, but still tractable. Other problems, even simple ones, have no known lower bounds.", "aSentId": 20, "answer": "Okay, even if you were talking about comparison sorting as a whole, you obviously still see the imbalance.  The upper bound is a \"there exists\" proof:  find one example, like mergesort, and you have proven the upper bound.  The lower bound is a \"not there exists\" or \"for all, not\" proof:  you need to prove a statement for all algorithms, discovered or not.\n\nFor many, many problems, there are known lower bounds, it is just that they probably aren't tight.  So, for many problems where you are given data with no structure, you might expect a linear lower bound.  But, a linear lower bound isn't too satisfying if the best known algorithm is n^2.\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 1, "question": "Big-O Misconceptions", "aSentId": 22, "answer": "Number one is a HUGE pet peeve of mine. &gt;:(", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 23, "question": "Number one is a HUGE pet peeve of mine. &gt;:(", "aSentId": 24, "answer": "Perhaps use f(x) \u2208 O(g(x))?  I've rarely seen it written as equality", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 25, "question": "Perhaps use f(x) \u2208 O(g(x))?  I've rarely seen it written as equality", "aSentId": 26, "answer": "In my school's CS Foundations I &amp; II classes, professors will count off if you use '='.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 1, "question": "Big-O Misconceptions", "aSentId": 28, "answer": "Very helpful for me. Thanks.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 31, "question": "I'm not sure I agree with misconception 1, please correct me if I'm wrong. \n\n&gt;is a widespread travestry. If you take it at face value, you can deduce that since 5n and 3n are both equal to O(n), then 3n must be equal to 5n and so 3=5.\n\nWell, we're not comparing two functions though, it's not f(n) = g(n) it's f(n) = O(g(n)) which is different. Furthermore by definition of big O we're ignoring multiplicative constants and lower order terms (i.e. it's asymptotic). I read f(n) = O(g(n)) as *f(n) is Big O of g(n)*. \n\n&gt;O(f)={g\u2223g(n)\u2264Mf(n) for some M&gt;0 for large n}.\n\nOr f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\n&gt;Misconception 3: \u201cBig-O is a Statement About Time\u201d\n\nIt's confusing when books or articles use phrases such as \"The running time of &lt;algorithm&gt; is O(n log n)\". One thinks that Big O is about running time which is not technically correct as you said. \n\n&gt;Misconception 4: Big-O Is About Worst Case\n\nIn the example you gave, if someone says randomized Quicksort has complexity O(n log n) they are wrong, because that's average case. You can measure best case, average case and worst case analysis with Big O, but it still is a worst case complexity in all three cases. ", "aSentId": 32, "answer": "&gt;Well, we're not comparing two functions though\n\nIf I say f(n) = O(g(n)), and f'(n) = O(g(n)), since equality is transitive, that should imply f(n) = f'(n).\n\n&gt;Or f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\nBig-O notation only cares about asymptotic bounds, so your definition is wrong. \"For large n\" is non-optional.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 31, "question": "I'm not sure I agree with misconception 1, please correct me if I'm wrong. \n\n&gt;is a widespread travestry. If you take it at face value, you can deduce that since 5n and 3n are both equal to O(n), then 3n must be equal to 5n and so 3=5.\n\nWell, we're not comparing two functions though, it's not f(n) = g(n) it's f(n) = O(g(n)) which is different. Furthermore by definition of big O we're ignoring multiplicative constants and lower order terms (i.e. it's asymptotic). I read f(n) = O(g(n)) as *f(n) is Big O of g(n)*. \n\n&gt;O(f)={g\u2223g(n)\u2264Mf(n) for some M&gt;0 for large n}.\n\nOr f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\n&gt;Misconception 3: \u201cBig-O is a Statement About Time\u201d\n\nIt's confusing when books or articles use phrases such as \"The running time of &lt;algorithm&gt; is O(n log n)\". One thinks that Big O is about running time which is not technically correct as you said. \n\n&gt;Misconception 4: Big-O Is About Worst Case\n\nIn the example you gave, if someone says randomized Quicksort has complexity O(n log n) they are wrong, because that's average case. You can measure best case, average case and worst case analysis with Big O, but it still is a worst case complexity in all three cases. ", "aSentId": 34, "answer": "As for misconception 1, the point of the author is that equality, at the very least, refers to an equivalence relation that's (i) commutative: A = B if and only if B = A; (ii) transitive: A = B and B = C implies A = C; and (iii) reflexive: A = A for all A.  If you assume this to be true, you can legitimately derive outrageously false statements as the author has done.  The way you think about/read the statement as \"f(n) is Big-O of g(n)\" is correct, but that's not what equality says.  To express this statement with notation we shouldn't use equality or an equivalence relation because this isn't a symmetric/transitive relationship.  That's why element-of or subset (dependeing on the context) makes much more sense.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 31, "question": "I'm not sure I agree with misconception 1, please correct me if I'm wrong. \n\n&gt;is a widespread travestry. If you take it at face value, you can deduce that since 5n and 3n are both equal to O(n), then 3n must be equal to 5n and so 3=5.\n\nWell, we're not comparing two functions though, it's not f(n) = g(n) it's f(n) = O(g(n)) which is different. Furthermore by definition of big O we're ignoring multiplicative constants and lower order terms (i.e. it's asymptotic). I read f(n) = O(g(n)) as *f(n) is Big O of g(n)*. \n\n&gt;O(f)={g\u2223g(n)\u2264Mf(n) for some M&gt;0 for large n}.\n\nOr f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\n&gt;Misconception 3: \u201cBig-O is a Statement About Time\u201d\n\nIt's confusing when books or articles use phrases such as \"The running time of &lt;algorithm&gt; is O(n log n)\". One thinks that Big O is about running time which is not technically correct as you said. \n\n&gt;Misconception 4: Big-O Is About Worst Case\n\nIn the example you gave, if someone says randomized Quicksort has complexity O(n log n) they are wrong, because that's average case. You can measure best case, average case and worst case analysis with Big O, but it still is a worst case complexity in all three cases. ", "aSentId": 36, "answer": "Regarding Misconception 4, when talking about randomized algorithms, it's not common to consider the worst/best/average cases.  As my undergrad classes taught me, QuickSort runs in expected Theta(n log(n)) steps.\n\nHowever, that's not really what's cool about QuickSort.  Instead, as a host of randomness in computing graduate courses explained to me, the probability that it will take more than Theta(n log(n)) steps is extremely low, and drops off exponentially as n grows.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 38, "question": "The article misses my biggest pet peeve about Big O notation: using *f(n) \u2208 O(g(n))* instead of *f \u2208 O(g)*.  *f* is a function, *f(n)* is not.", "aSentId": 39, "answer": "*f(n)* is accepted mathematical notation for a function with one free variable. There's no problem here.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 38, "question": "The article misses my biggest pet peeve about Big O notation: using *f(n) \u2208 O(g(n))* instead of *f \u2208 O(g)*.  *f* is a function, *f(n)* is not.", "aSentId": 41, "answer": "haha you must hate all the comments on this post", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 38, "question": "The article misses my biggest pet peeve about Big O notation: using *f(n) \u2208 O(g(n))* instead of *f \u2208 O(g)*.  *f* is a function, *f(n)* is not.", "aSentId": 43, "answer": "So how would you write *f(n) \u2208 O(n log n)*? Are you required to write it in two lines, *g(n) = n log n; f \u2208 O(g)*?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 44, "question": "So how would you write *f(n) \u2208 O(n log n)*? Are you required to write it in two lines, *g(n) = n log n; f \u2208 O(g)*?", "aSentId": 45, "answer": "*f \u2208 O(n \u21a6 n lg n)*", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 49, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 50, "answer": "It's not unreasonable or uncommon to neglect the asymptotic cost of dealing with big numbers. We do this all the time when indexing arrays, for instance. You could argue that traversing from one array cell to the next is not O(1) because incrementing the index becomes more expensive as the number of digits in the index grows. (That darn carry from 999...999 to 1000...000.) Ultimately, we sacrifice some pedantry for the sake of simplicity, whether it's a uniform memory model or an arbitrarily big hash function.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 51, "question": "It's not unreasonable or uncommon to neglect the asymptotic cost of dealing with big numbers. We do this all the time when indexing arrays, for instance. You could argue that traversing from one array cell to the next is not O(1) because incrementing the index becomes more expensive as the number of digits in the index grows. (That darn carry from 999...999 to 1000...000.) Ultimately, we sacrifice some pedantry for the sake of simplicity, whether it's a uniform memory model or an arbitrarily big hash function.", "aSentId": 52, "answer": "But a) the whole point of big-O is that we don't get to place bounds on problem size, and b) for that reason I have a hard time saying that memory access is constant either!\n\nSo (and I admit this is changing the question a bit), what if I'm in contexts where I'm judged by someone by-the-book and I also don't want to say anything I consider to be wrong?\n\n- \"It's constant across real-world ranges.\" ?\n- \"Within standard memory limits, it's constant.\" ?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 53, "question": "But a) the whole point of big-O is that we don't get to place bounds on problem size, and b) for that reason I have a hard time saying that memory access is constant either!\n\nSo (and I admit this is changing the question a bit), what if I'm in contexts where I'm judged by someone by-the-book and I also don't want to say anything I consider to be wrong?\n\n- \"It's constant across real-world ranges.\" ?\n- \"Within standard memory limits, it's constant.\" ?", "aSentId": 54, "answer": "\"This is O(1) presuming the cost of the hash function does not materially depend on n.\"\n\nYou do get to put practical limits on things. All the time. You have to.  And, more importantly, these sorts of limits are part of the language that people use to communicate ideas.  By hauling off and redefining the constraints for O(1) for yourself, you make it harder to actually communicate.  (Also, it makes you look like a bit of a twat).\n\nIf you're feeling very strongly about it, your best bet is not to try changing people's minds by fiat, but rather to do some research and prove that the effect should be of concern.  It is very well possible that e.g. memory should not be considered O(1) access time. But do a literature study first, and then do some research before spending time trying to convince people e.g. that the technically non constant big-O of ADD(a,b) is terribly important to how they design their sort algorithm.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 49, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 56, "answer": "Usually, long before you hit the \"extremely high values\", you have either...\n\n* Run out of ram available to real world processes.\n* Or used up more CPU time than you have patience for.\n\nAn awful lot of \"O\" arithmetic quietly assumes \"infinite\" computers running finite problems, for the simple reason trying to fully account for the complexity of real computers is too hard.\n\nBy the time you get into cache effects and swapping and multiple cores and NUMA and.... Sigh! It's plain easier and more reliable to measure for realistic loads.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 49, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 58, "answer": "&gt; hashtable lookup is constant (amortized)\n\nwell, the word \"amortized\" means something. Look it up.\n\nFor 1 operation, you are right, it's O(logn), or O(n). But for n operations, does it mean you may require up to O(n logn)? No, you still require O(n).\n\nFor example dynamic array: \"Insertion or removal of elements at the end - **amortized** constant O(1)\". For 1 insertion it may trigger reallocation of the whole array, so yes it's O(n) for 1 insertion. But for n insertions it still requires O(n), not O(n^2 ). Assume you insert  9 elements into an empty array, when the array is full, you reallocate it, double its size. So the cost are: \n\n1. 1 (1/1)\n1. 2 (realloc 1 element) (2/2)\n1. 3 (realloc 2 element) (3/4)\n1. 1 (4/4)\n1. 5 (realloc 4 elements) (5/8)\n1. 1 (6/8)\n1. 1 (7/8)\n1. 1 (8/8)\n1. 9 (realloc 8 elements) (9/16)\n\nThe total cost for 9 insertions is 9 + (1 + 2 + 4 + 8) = 24.\n\nYou may generalize it for n insertions, where n = 2^k + 1:\n\nn + (1 + 2 + 4 + ... + 2^k )  \n= n + (2^k+1 - 1) / (2 - 1)  \n= n + 2 * 2^k - 1.  \nBut n = 2^k + 1, so 2^k = n - 1, so our total cost are  \nn + 2(n - 1) - 1  \n= 3n - 3.  \nSo for n insertions it requires only O(n), not O(n^2 ). Therefore it is O(1) **amortized**", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 62, "question": "If you are using a linked-list implementation of a HashTable, yes, lookup can be, in the worst case O(n). However, if you are using a statically sized HashTable with replacement (1 item per bucket), you will have O(1) for lookup. ", "aSentId": 63, "answer": "But that would be a special case, not a general must-keep-all-keys hashtable.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 67, "question": "Advice about how to study CS", "aSentId": 68, "answer": "My advice is to take classes that interest you and try and learn as much as you can while in school. You'll never be in a better place to learn than you are right now. If there's stuff that interests you right now, pursue it right now -- hopefully your school has ways to support you in that, and even if not you're in the right environment.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 70, "question": "Fixing a typo is probably the most rewarding and worst feeling ever.", "aSentId": 71, "answer": "These bugs that take the longest to find tend to have the simplest and many times, stupidest, solutions.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 74, "question": "Looking to learn my first language -- any recommendations/course of action to learning it quickly?", "aSentId": 75, "answer": "APL", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 74, "question": "Looking to learn my first language -- any recommendations/course of action to learning it quickly?", "aSentId": 77, "answer": "Learn Haskell now. It's very mathematical, and even if it's not your main language, it will teach you to code in a way that improves the quality of code you write in other languages. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 78, "question": "Learn Haskell now. It's very mathematical, and even if it's not your main language, it will teach you to code in a way that improves the quality of code you write in other languages. ", "aSentId": 79, "answer": "Maybe I'm naive. because I don't have a strong mathematical background and my first language was imperative, but I'd consider a purely functional language, like Haskell, to be intermediate level. Maybe beginner for someone with a strong mathematical grounding.\n\nAt least with functional languages that aren't pure you're free to learn in an imperative/OO way, even if that's not how the language was 'intended' to be used.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 81, "question": "Optimization of multiple attributes", "aSentId": 82, "answer": "The isn't an optimization problem. You have no \"objective\" function that you're either maximizing or minimizing. You simply need to stable sort by attribute. The order of which attribute sorts will determine the \"weight\" assigned to each attribute.\n\nIf you need something to google, then google radix sort. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 84, "question": "Interesting look on those crazy interviews everyone talks about", "aSentId": 85, "answer": "I still hear quite a bit about the shockingly large fraction of interviewees who can't solve the FizzBuzz problem. Anybody who's conducted interviews like this like to chime in? Is it mostly people who haven't held a software engineering job before?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 86, "question": "I still hear quite a bit about the shockingly large fraction of interviewees who can't solve the FizzBuzz problem. Anybody who's conducted interviews like this like to chime in? Is it mostly people who haven't held a software engineering job before?", "aSentId": 87, "answer": "It's important to keep in mind that competent engineers who are easily performing in a stable role tend to not be the kind of people who are constantly going around applying for positions. Incompetent programmers make up a much greater percentage of the applicant pool than they do in pool of all programmers. Even if they held a job before, it's still going to be tougher for them to get a new one if their skills don't qualify them.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 89, "question": "A Theory of Changes for Higher-Order Languages [pdf]", "aSentId": 90, "answer": "That indeed looks extremely cool and useful. Wouldn't this improve the complexity of functional reactive programs that re-render a tree every frame... or of just games with scenegraphs, for that matter?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 89, "question": "A Theory of Changes for Higher-Order Languages [pdf]", "aSentId": 92, "answer": "Damn, I just started reading, this looks super cool.\n\nE: I'd love to use this in build systems.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 93, "question": "Damn, I just started reading, this looks super cool.\n\nE: I'd love to use this in build systems.", "aSentId": 94, "answer": "We'll have to wait until they scale this technique up to handle languages with effects, which can be a long, hard road to travel :-\\", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 89, "question": "A Theory of Changes for Higher-Order Languages [pdf]", "aSentId": 96, "answer": "Are the changes normalized? i.e if two changes results in same output, can we see know that the outputs are exactly the same?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 98, "question": "Is (was?) there a Carl Sagan/NDT of CS?", "aSentId": 99, "answer": "Donald Knuth opened the doors to algorithms for many and still does ground breaking work. He writes voluminously and his work is very readable. We had books before there were YouTube videos.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 98, "question": "Is (was?) there a Carl Sagan/NDT of CS?", "aSentId": 101, "answer": "No, nobody with anywhere close to the penetration of Sagan or NDT.  However, I really don't see a reason why there couldn't be . . . there should be.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 98, "question": "Is (was?) there a Carl Sagan/NDT of CS?", "aSentId": 103, "answer": "My guess for a somewhat analogous popularizer or public advocate of computer science would be Grace Hopper. She didn't have nearly the reach or cultural exposure, but she did get out there as a touring lecturer later in her life. I remember she visited my middle school once, back in the 1980s.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 105, "question": "Steve Jobs\n\nedit: Sheldon Cooper", "aSentId": 106, "answer": "Steve Jobs has contributed about as much to the field of computer science as an actor in a Viagra commercial has to the field of biomedical research.\n\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 107, "question": "Steve Jobs has contributed about as much to the field of computer science as an actor in a Viagra commercial has to the field of biomedical research.\n\n", "aSentId": 108, "answer": "Which reminds one that when Steve Jobs died, the world went on a grief orgy that lasted for months; and when Dennis Ritchie died a week later, it barely made the papers. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 109, "question": "Which reminds one that when Steve Jobs died, the world went on a grief orgy that lasted for months; and when Dennis Ritchie died a week later, it barely made the papers. ", "aSentId": 110, "answer": "That's the joke", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 107, "question": "Steve Jobs has contributed about as much to the field of computer science as an actor in a Viagra commercial has to the field of biomedical research.\n\n", "aSentId": 112, "answer": "How much has Neil Degrasse Tyson done for astronomy?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 113, "question": "How much has Neil Degrasse Tyson done for astronomy?", "aSentId": 114, "answer": "He is an active educator in the field, and it helps to get people into astronomy when it's no longer some super academic and fancy field but something that can be understood rather easily. Making it accessible is more important than anything.\n\nMaybe you can argue that Steve Jobs has inspired people to go into technology, but IMO even Mark Zuckerberg and Google's founders are better at that than Steve Jobs. Or if you actually know Apple's history, then it's Steve Wozniak's influence which overshadows that of Steve Jobs. And nobody beats out Bill Gates in that department, anyway.\n\nBut what they all lack is the Carl Sagan educational+enthusiasm factor.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 116, "question": "Cs advice", "aSentId": 117, "answer": "You are asking the wrong question. \n\nThere are three things you can get out of a CS education: understanding and knowledge useful for work in the field, a glide path into a high paying job and a network of relations and acquaintances.\n\nEach of those is important in its own way. For the first, CS unfortunately is more of a multiplier than an enabler (ie if you really don't understand math, science or programming, you won't get much out of CS program), so in a way it matters relatively little where you go, except that if you have what it takes to be stellar and  want to stay in academia/research, you ought to go to the A-class universities which attract other prodigies due to their reputation. A single great mentor/teacher will matter more than which institution you chose. Unfortunately, getting the attention of one will come down to a certain extent to luck, but check the departmental websites for people you would consider candidates for that role.\n\nIf you're into it for the money, any accredited degree will be \"good enough\" to get through a door in the current job market, the rest will be in the interview; it'll matter what field you have focused on and -in economic terms- how much you paid for it, so the cheapest alternative with sufficient street cred will likely suffice.\n\nThe last - people you get to know, be they in CS or outside of the field - will play a lasting role in who you spend your life with and what opportunities you might become aware of. I have no information to gauge which of the two institutions would be more beneficial, but try to get a feel for this as well (visit?).\n\nSo, the question comes down to \"it depends\", and specifically to \"what do you really want and what can you invest to make that happen\"?\n\nApologies for note being too helpful here ;)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 116, "question": "Cs advice", "aSentId": 119, "answer": "university of Tennessee Knoxville.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 120, "question": "university of Tennessee Knoxville.", "aSentId": 121, "answer": "Why do u say that? ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 123, "question": "Looking for a secondary programming hobby.", "aSentId": 124, "answer": "/r/programming", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 128, "question": "Can someone recommend a parsing library?", "aSentId": 129, "answer": "If you know how to build a grammar then you could use flex and bison. It's honestly not that hard to use once you get everything setup and you know C", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 128, "question": "Can someone recommend a parsing library?", "aSentId": 131, "answer": "I second the Prolog suggestion:\n\nProlog has a built-in mechanism for parsing, called *definite clause grammars* (DCGs), with which you can parse extremely comfortably. You can use library(pio) to apply such DCGs to files, and parse them without having to read the whole file at once.\n\nIn fact, one of the very first applications of Prolog was parsing natural language sentences in the context of a translation project. So, Prolog is very suitable for this task.\n\nSee also one of the [DCG SWISH examples](http://swish.swi-prolog.org/example/grammar.pl), which you can try online.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 133, "question": "Why Do We Care About Getting Asymptotically Good Algorithms?", "aSentId": 134, "answer": "Asymptotic runtime is the only thing that makes sense to measure. If you only care about runtime for problems less than a certain size, a lookup table is always the fastest algorithm as it works in constant time. And if you care about the constant that your algorithm is multiplied by (2n vs 200n), then your runtime depends on the hardware you are running on, the language you are writing in, and how good the compiler you are using is.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 135, "question": "Asymptotic runtime is the only thing that makes sense to measure. If you only care about runtime for problems less than a certain size, a lookup table is always the fastest algorithm as it works in constant time. And if you care about the constant that your algorithm is multiplied by (2n vs 200n), then your runtime depends on the hardware you are running on, the language you are writing in, and how good the compiler you are using is.", "aSentId": 136, "answer": "How come the platform affects on the runtime behavior by a factor?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 137, "question": "How come the platform affects on the runtime behavior by a factor?", "aSentId": 138, "answer": "Several reasons, but the most annoying one might be that different operations take different amounts of times on different platforms. Lets say you have two solutions to a problem and are deciding which algorithm is more efficient, one that uses\n\n    A. 5n vector lookups and 20n integer additions, or  \n    B. 10n vector lookups and 5n integer additions\n\nNow to know which one is faster, all of these matter:\n\n * What's the difference between CPU speed and RAM/cache speed? With a fast CPU but slow RAM/cache, A will be faster. With a slow CPU but fast RAM/cache, B will be faster.\n * How efficient is your languages vector implementation? All vector lookups take at least one integer addition, but some might take more than one, and some do bounds-checking.\n * Is your compiler going to inline the vector lookup calls? This might make the 5 additional vector lookups faster than the 15 additions we get to drop\n\nOn the other hand if you ignore constants, it makes everything very easy. Just count the number of constant-time operations your algorithm takes. It doesn't matter if some of those constant-time operations are slower or faster than others.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 139, "question": "Several reasons, but the most annoying one might be that different operations take different amounts of times on different platforms. Lets say you have two solutions to a problem and are deciding which algorithm is more efficient, one that uses\n\n    A. 5n vector lookups and 20n integer additions, or  \n    B. 10n vector lookups and 5n integer additions\n\nNow to know which one is faster, all of these matter:\n\n * What's the difference between CPU speed and RAM/cache speed? With a fast CPU but slow RAM/cache, A will be faster. With a slow CPU but fast RAM/cache, B will be faster.\n * How efficient is your languages vector implementation? All vector lookups take at least one integer addition, but some might take more than one, and some do bounds-checking.\n * Is your compiler going to inline the vector lookup calls? This might make the 5 additional vector lookups faster than the 15 additions we get to drop\n\nOn the other hand if you ignore constants, it makes everything very easy. Just count the number of constant-time operations your algorithm takes. It doesn't matter if some of those constant-time operations are slower or faster than others.", "aSentId": 140, "answer": "This assumes the Church-Turing hypothesis to be true, right?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 141, "question": "This assumes the Church-Turing hypothesis to be true, right?", "aSentId": 142, "answer": "It actually assumes something stronger, that all computers have the same set of operations they can do it constant time. Which is true for just about every physical computer we've had over the past hundred years, but actually false for quantum computers and turing machines.\n\nIf you want to bring other types of computers in, then you can't even talk about O(n^(2)) vs O(n^(3)), as for example turing machines have slower memory access than conventional computers. If the Complexity-Theoretic Church\u2013Turing thesis is true, then if a problem is solvable in polynomial time on one computer, it's solvable in polynomial time on all computers. But not necessarily the same polynomial.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 135, "question": "Asymptotic runtime is the only thing that makes sense to measure. If you only care about runtime for problems less than a certain size, a lookup table is always the fastest algorithm as it works in constant time. And if you care about the constant that your algorithm is multiplied by (2n vs 200n), then your runtime depends on the hardware you are running on, the language you are writing in, and how good the compiler you are using is.", "aSentId": 144, "answer": "Lookup table might be fast but uses huge amounts of space. What if I care about runtime for problems less than a certain size, but that size is fairly large?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 145, "question": "Lookup table might be fast but uses huge amounts of space. What if I care about runtime for problems less than a certain size, but that size is fairly large?", "aSentId": 146, "answer": "I'm speaking from a theoretical viewpoint. If you want to come up with a rigorous definition for what makes one algorithm faster than another, runtime on small problem sizes won't work, because then you need to bring allowable program size into the equation, and program size will depend on your platform and other things you would rather abstract away when doing theoretical computer science.\n\nOf course in practice you have a valid concern, which is why we often use algorithms with a slower big-O time, that are actually faster for any reasonably input size.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 147, "question": "I'm speaking from a theoretical viewpoint. If you want to come up with a rigorous definition for what makes one algorithm faster than another, runtime on small problem sizes won't work, because then you need to bring allowable program size into the equation, and program size will depend on your platform and other things you would rather abstract away when doing theoretical computer science.\n\nOf course in practice you have a valid concern, which is why we often use algorithms with a slower big-O time, that are actually faster for any reasonably input size.", "aSentId": 148, "answer": "Caching is such a dominate force for most workloads, that we are going to have to start accounting for that.  \n\nIf you just went with big O notation you might choose a linked list for its fast insert/remove time, but in practice vector is a much better choice because of the vastly improved data locality.  ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 135, "question": "Asymptotic runtime is the only thing that makes sense to measure. If you only care about runtime for problems less than a certain size, a lookup table is always the fastest algorithm as it works in constant time. And if you care about the constant that your algorithm is multiplied by (2n vs 200n), then your runtime depends on the hardware you are running on, the language you are writing in, and how good the compiler you are using is.", "aSentId": 150, "answer": "A lookup table is not always faster due to the effects of hardware (consider caching) . ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 151, "question": "A lookup table is not always faster due to the effects of hardware (consider caching) . ", "aSentId": 152, "answer": "Exactly, hardware-dependent obstacles are why we have to ignore constant-time factors when talking about \"the fastest\" algorithm. If you take caching into account and try to optimize for constant fators, the fastest algorithm on one computer won't be the fastest on another computer.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 133, "question": "Why Do We Care About Getting Asymptotically Good Algorithms?", "aSentId": 154, "answer": "This is pretty rambling, and there are better introductory resources to complexity theory. Also, quicksort is n^(2), yet it's still faster than bubble sort, so it's a piss poor example.\n\nI think the reason we care about asymptotic behaviour, especially in the sub-quadratic space, is because if we know the algorithm runs well on small inputs, if the time complexity is less than quadratic we can expect it to not blow up in our faces. Binary search will always scale.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 155, "question": "This is pretty rambling, and there are better introductory resources to complexity theory. Also, quicksort is n^(2), yet it's still faster than bubble sort, so it's a piss poor example.\n\nI think the reason we care about asymptotic behaviour, especially in the sub-quadratic space, is because if we know the algorithm runs well on small inputs, if the time complexity is less than quadratic we can expect it to not blow up in our faces. Binary search will always scale.", "aSentId": 156, "answer": "&gt; I think the reason we care about asymptotic behaviour, especially in the sub-quadratic space, is because if we know the algorithm runs well on small inputs, if the time complexity is less than quadratic we can expect it to not blow up in our faces.\n\nSeems like you're trying to answer the question in the title, but you're just restating the motivation behind the article.\n\nIf the only reason to care about asymptotic behavior is to know that testing with small inputs helps us know large inputs are feasible, then why do we care about the difference between O(n^(10)) and O(2^(n))? If that's the only reason, then these two would be practically identical.\n\nBut it's not the only reason, as the rest of the article covers.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 155, "question": "This is pretty rambling, and there are better introductory resources to complexity theory. Also, quicksort is n^(2), yet it's still faster than bubble sort, so it's a piss poor example.\n\nI think the reason we care about asymptotic behaviour, especially in the sub-quadratic space, is because if we know the algorithm runs well on small inputs, if the time complexity is less than quadratic we can expect it to not blow up in our faces. Binary search will always scale.", "aSentId": 158, "answer": "It's technically possible to force quicksort to run in O(n log n) for any input, but you get some ugly constants", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 159, "question": "It's technically possible to force quicksort to run in O(n log n) for any input, but you get some ugly constants", "aSentId": 160, "answer": "Whew! I was like \"but I thought quicksort could be done in O(n log n)...\" Thanks for the clarification! ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 161, "question": "Whew! I was like \"but I thought quicksort could be done in O(n log n)...\" Thanks for the clarification! ", "aSentId": 162, "answer": "The regular implementation is typically O(n log n) but can run in O(n^2) in some cases.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 167, "question": "The Limits of Quantum Computing [pdf]", "aSentId": 168, "answer": "In case anyone doesn't know Scott Aaronson, he's basically the public face for the entire field of quantum complexity.  He's a very big deal, and has been on and off one of the most vocal D-Wave critics, though largely from a point of view of aggregating other people's (Lidar, Troyer, ..) findings.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 169, "question": "In case anyone doesn't know Scott Aaronson, he's basically the public face for the entire field of quantum complexity.  He's a very big deal, and has been on and off one of the most vocal D-Wave critics, though largely from a point of view of aggregating other people's (Lidar, Troyer, ..) findings.", "aSentId": 170, "answer": "He also maintains an oftentimes hilarious blog with many old articles which are worth reading, for example the proof of Rosser's theorem via Turing machines: https://google.com/?q=site:scottaaronson.com/blog", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 167, "question": "The Limits of Quantum Computing [pdf]", "aSentId": 172, "answer": "This is quite old (2008), but still very interesting. Have there been any significant changes to this view since?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 174, "question": "Proving a language is not Turing-recognizable by reduction", "aSentId": 175, "answer": "Because R is the intersection of co-RE and RE, if you can show an undecidable language is in co-RE (i.e. its complement is recognizable) then it is necessarily not in RE.\n\nNote that your language D *is* recognizable (there is a semi-decision procedure for it), so reductions involving D (either to or from) will only help in showing that your given language is undecidable (i.e. with a reduction *from* D).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 176, "question": "Because R is the intersection of co-RE and RE, if you can show an undecidable language is in co-RE (i.e. its complement is recognizable) then it is necessarily not in RE.\n\nNote that your language D *is* recognizable (there is a semi-decision procedure for it), so reductions involving D (either to or from) will only help in showing that your given language is undecidable (i.e. with a reduction *from* D).", "aSentId": 177, "answer": "_--__'s answer is exactly correct, but perhaps for somebody new to the topic, it is hard to decipher.  There are two approaches.  \n1) First, show that your language is not decidable, which you say you have examples of.  Next, show that your language is co-recognizable.  Now, if it were both co-recognizable, and recognizable, then it would be decidable, so it can't be both if it isn't decidable.\n\n2) Reduce a non-recognizable language to it.  In your case, you are trying to reduce a recognizable (but non-co-recognizable) language to it, so there had better be some kind of negation in your reduction.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 174, "question": "Proving a language is not Turing-recognizable by reduction", "aSentId": 179, "answer": "&gt; Most of the things I've read show how to prove a language is not decidable, but not recognizable.\n\nThat's \"if I could decide X, then I could also decide Y, but we already know Y is undecidable.\"\n\n&gt; How would it be different?\n\nChange it to \"if I could recognize X, then I could also recognize Y, but we already know Y is unrecognizable.\" So show how a \"semi-oracle\" for X can let you recognize Y.\n\n&gt; D reduces to ATM.\n\nOnly in decidability.\n\n&gt; Plus, how could I change this to show ATM is unrecognizable?\n\nYou can't. You're doing the reduction in the wrong direction for that (knowing that an oracle for some hard problem makes this problem easy does not mean this problem is hard). Also ATM is recognizable.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 174, "question": "Proving a language is not Turing-recognizable by reduction", "aSentId": 181, "answer": "Post on cstheory stackexchange.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 184, "question": "Uncertain about master degree and job", "aSentId": 185, "answer": "Well theres jobs for all of those fields.  Data engineering I feel is unessecary to specialize in, I would think most software at some point is going to interface to a database - you're going to learn that anyway and usually pretty quickly.\n\nI can only speak to Software Engineering - there are plenty of jobs.  I've never been without a job.  Rates are usually based on experience, I started at $45, now at around $100, and I expect to cap out around 120 or 130 - unless you get into some really specialized areas.\n\nI personally like embedded systems and do them for fun ( building a car counter at the moment , seems like there is so much traffic on my street ), but I have heard they pay less.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 186, "question": "Well theres jobs for all of those fields.  Data engineering I feel is unessecary to specialize in, I would think most software at some point is going to interface to a database - you're going to learn that anyway and usually pretty quickly.\n\nI can only speak to Software Engineering - there are plenty of jobs.  I've never been without a job.  Rates are usually based on experience, I started at $45, now at around $100, and I expect to cap out around 120 or 130 - unless you get into some really specialized areas.\n\nI personally like embedded systems and do them for fun ( building a car counter at the moment , seems like there is so much traffic on my street ), but I have heard they pay less.", "aSentId": 187, "answer": "&gt; Rates are usually based on experience, I started at $67, now at around $110, and I expect to cap out around 120 or 130 - unless you get into some really specialized areas.\n\nIs this per hour?  Thousands per year?  You need to specify units.  Also area for COL.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 188, "question": "&gt; Rates are usually based on experience, I started at $67, now at around $110, and I expect to cap out around 120 or 130 - unless you get into some really specialized areas.\n\nIs this per hour?  Thousands per year?  You need to specify units.  Also area for COL.", "aSentId": 189, "answer": "&gt; Is this per hour? Thousands per year? You need to specify units. Also area for COL.\n\nAll numbers are annual salary.  This is in Texas where COL is cheap, expect more in dense cities etc.\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 190, "question": "&gt; Is this per hour? Thousands per year? You need to specify units. Also area for COL.\n\nAll numbers are annual salary.  This is in Texas where COL is cheap, expect more in dense cities etc.\n", "aSentId": 191, "answer": "Hi, thanks for the comment.\nHowever I don't understand if $67, $110, ... are thousands per year?\n\nThanks you", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 184, "question": "Uncertain about master degree and job", "aSentId": 193, "answer": "Honestly, just do the one that you will enjoy the most. The soft.eng masters is probably a waste of time. If you get a dev position then they will teach you how to program properly. It's not the sort of thing you can learn in a university.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 194, "question": "Honestly, just do the one that you will enjoy the most. The soft.eng masters is probably a waste of time. If you get a dev position then they will teach you how to program properly. It's not the sort of thing you can learn in a university.", "aSentId": 195, "answer": "How does that work exactly?  All the postings I see seem to assume you've already built a bunch of stuff in undergrad and are a pretty competent programmer.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 196, "question": "How does that work exactly?  All the postings I see seem to assume you've already built a bunch of stuff in undergrad and are a pretty competent programmer.", "aSentId": 197, "answer": "YMMV\n\nYou have to know some programming (I presume you get this from comp.eng) but when we interview it's generally assumed that we'll have to teach juniors a lot. I think academic qualifications in soft.eng are pretty useless but of course it depends on the syllabus. Your time is probably better spent doing computer science / maths / stats / engineering. \n\nMy team is composed of mathematicians, physicist, economists, biologists etc. And almost all of them have learnt to program 'properly' on the job.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 198, "question": "YMMV\n\nYou have to know some programming (I presume you get this from comp.eng) but when we interview it's generally assumed that we'll have to teach juniors a lot. I think academic qualifications in soft.eng are pretty useless but of course it depends on the syllabus. Your time is probably better spent doing computer science / maths / stats / engineering. \n\nMy team is composed of mathematicians, physicist, economists, biologists etc. And almost all of them have learnt to program 'properly' on the job.", "aSentId": 199, "answer": "I would agree with this 100%, focus on the STEM stuff - the programming can be taught on the job.\n\nAlso what are they teaching you in school??  Not programming it sounds like ?  Software is one of those things that is best learnt by doing - so just build as much software as you can.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 198, "question": "YMMV\n\nYou have to know some programming (I presume you get this from comp.eng) but when we interview it's generally assumed that we'll have to teach juniors a lot. I think academic qualifications in soft.eng are pretty useless but of course it depends on the syllabus. Your time is probably better spent doing computer science / maths / stats / engineering. \n\nMy team is composed of mathematicians, physicist, economists, biologists etc. And almost all of them have learnt to program 'properly' on the job.", "aSentId": 201, "answer": "&gt; STEM\n\nThanks!", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 202, "question": "&gt; STEM\n\nThanks!", "aSentId": 203, "answer": "You can learn \"how to program\" quite easily, but the good programmers are the ones that understands how everything works underneath the software. How does the CPU treat threads and what kind of operations are CPU or memory expensive? How do you efficiently access memory pages and reduce CPU stalls? You wont learn much from copy-pasting some code from StackOverflow. I'm not saying hands-on experience isn't worth something, but you can get much farther with a masters degree.\n\nI have worked with people with different degrees in the field of Computer Science. The true IT-gurus are always the ones with a masters degree. But they are also the ones that made it their hobby to try out everything.\n\nHaving a masters degree has one down site, though. You start to realise how little your colleges know about the stuff they do for a living.", "corpus": "reddit"}]
