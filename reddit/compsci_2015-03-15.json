[{"docID": "t5_2qhmr", "qSentId": 43758, "question": "Big-O Misconceptions", "aSentId": 43759, "answer": "Interesting, at my university, we primarily used theta. It really forced you to think harder about runtime. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43760, "question": "Interesting, at my university, we primarily used theta. It really forced you to think harder about runtime. ", "aSentId": 43761, "answer": "The thing is, proving lower bounds is MUCH HARDER than proving upper bounds! Looking at comparison sorts, the `n log n` upper bound is much easier to derive than the `n log n` lower bound.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43762, "question": "The thing is, proving lower bounds is MUCH HARDER than proving upper bounds! Looking at comparison sorts, the `n log n` upper bound is much easier to derive than the `n log n` lower bound.", "aSentId": 43763, "answer": "That's one of the reasons my algorithm class was unusually hard. Some algorithms wouldn't fit into theta, so we would have to prove both O and Omega. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43762, "question": "The thing is, proving lower bounds is MUCH HARDER than proving upper bounds! Looking at comparison sorts, the `n log n` upper bound is much easier to derive than the `n log n` lower bound.", "aSentId": 43765, "answer": "The difficulty of those statements isn't fair to compare.  The upper bound is the worst case runtime of a specific algorithm.  So, once you have an algorithm like mergesort, we can see that it takes n lg n time.  The lower bound is a lower bound on the general problem of comparison based sorting.  For any comparison based sorting algorithm, it cannot have worst case runtime better than order n log n.\nFor a particular algorithm, proving it's upper and lower bounds might not be too hard from each other, but here, you are comparing the upper bound of one algorithm, vs. proving the non-existence of algorithms with worst-case runtimes of less than n log n.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43766, "question": "The difficulty of those statements isn't fair to compare.  The upper bound is the worst case runtime of a specific algorithm.  So, once you have an algorithm like mergesort, we can see that it takes n lg n time.  The lower bound is a lower bound on the general problem of comparison based sorting.  For any comparison based sorting algorithm, it cannot have worst case runtime better than order n log n.\nFor a particular algorithm, proving it's upper and lower bounds might not be too hard from each other, but here, you are comparing the upper bound of one algorithm, vs. proving the non-existence of algorithms with worst-case runtimes of less than n log n.", "aSentId": 43767, "answer": "In both cases, I was actually referring to the general problem of comparison sorting.\n\nThe \"unfair\" issue here is exactly why it's harder to show lower bounds than upper bounds. To show an upper bound, you find an algorithm that solves your problem, and you find out its time complexity. To show a lower bound, you need to show that *there can be no algorithm solving this problem which takes less time than this.*\n\nThe easiest example I can think of is showing that searching an unsorted array takes at least linear time, because it has to look at every single element of the array in the worst case. Comparison sorts are a harder problem, but still tractable. Other problems, even simple ones, have no known lower bounds.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43768, "question": "In both cases, I was actually referring to the general problem of comparison sorting.\n\nThe \"unfair\" issue here is exactly why it's harder to show lower bounds than upper bounds. To show an upper bound, you find an algorithm that solves your problem, and you find out its time complexity. To show a lower bound, you need to show that *there can be no algorithm solving this problem which takes less time than this.*\n\nThe easiest example I can think of is showing that searching an unsorted array takes at least linear time, because it has to look at every single element of the array in the worst case. Comparison sorts are a harder problem, but still tractable. Other problems, even simple ones, have no known lower bounds.", "aSentId": 43769, "answer": "What is the upper bound on general comparison-based sorting? I would assume it's O(n^(2)), but are there any correct sorting algorithms which are O(n^(3)) that don't use any unnecessary steps?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43770, "question": "What is the upper bound on general comparison-based sorting? I would assume it's O(n^(2)), but are there any correct sorting algorithms which are O(n^(3)) that don't use any unnecessary steps?", "aSentId": 43771, "answer": "comparison sorting is tightly bounded by n log n, both above and below. n^2 is a looser upper bound.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43772, "question": "comparison sorting is tightly bounded by n log n, both above and below. n^2 is a looser upper bound.", "aSentId": 43773, "answer": "What about quicksort? That has an upper bound of n^2 yet is a comparison based sorting algorithm.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43770, "question": "What is the upper bound on general comparison-based sorting? I would assume it's O(n^(2)), but are there any correct sorting algorithms which are O(n^(3)) that don't use any unnecessary steps?", "aSentId": 43775, "answer": "You can always build a worse algorithm, so there is no upper bound for the problem.  Worst-case comparison based sorting takes Omega(n lg n).  There exist algorithms that solve the problem in O(n lg n).  By those two statements, we say that the PROBLEM is an n lg n problem.  (We can use asymptotic notation to talk about the complexity of an algorithm, and also a problem as a whole.  Here, we are going with the latter.)\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43768, "question": "In both cases, I was actually referring to the general problem of comparison sorting.\n\nThe \"unfair\" issue here is exactly why it's harder to show lower bounds than upper bounds. To show an upper bound, you find an algorithm that solves your problem, and you find out its time complexity. To show a lower bound, you need to show that *there can be no algorithm solving this problem which takes less time than this.*\n\nThe easiest example I can think of is showing that searching an unsorted array takes at least linear time, because it has to look at every single element of the array in the worst case. Comparison sorts are a harder problem, but still tractable. Other problems, even simple ones, have no known lower bounds.", "aSentId": 43777, "answer": "Okay, even if you were talking about comparison sorting as a whole, you obviously still see the imbalance.  The upper bound is a \"there exists\" proof:  find one example, like mergesort, and you have proven the upper bound.  The lower bound is a \"not there exists\" or \"for all, not\" proof:  you need to prove a statement for all algorithms, discovered or not.\n\nFor many, many problems, there are known lower bounds, it is just that they probably aren't tight.  So, for many problems where you are given data with no structure, you might expect a linear lower bound.  But, a linear lower bound isn't too satisfying if the best known algorithm is n^2.\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43758, "question": "Big-O Misconceptions", "aSentId": 43779, "answer": "Number one is a HUGE pet peeve of mine. &gt;:(", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43780, "question": "Number one is a HUGE pet peeve of mine. &gt;:(", "aSentId": 43781, "answer": "Perhaps use f(x) \u2208 O(g(x))?  I've rarely seen it written as equality", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43782, "question": "Perhaps use f(x) \u2208 O(g(x))?  I've rarely seen it written as equality", "aSentId": 43783, "answer": "In my school's CS Foundations I &amp; II classes, professors will count off if you use '='.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43758, "question": "Big-O Misconceptions", "aSentId": 43785, "answer": "Very helpful for me. Thanks.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43788, "question": "I'm not sure I agree with misconception 1, please correct me if I'm wrong. \n\n&gt;is a widespread travestry. If you take it at face value, you can deduce that since 5n and 3n are both equal to O(n), then 3n must be equal to 5n and so 3=5.\n\nWell, we're not comparing two functions though, it's not f(n) = g(n) it's f(n) = O(g(n)) which is different. Furthermore by definition of big O we're ignoring multiplicative constants and lower order terms (i.e. it's asymptotic). I read f(n) = O(g(n)) as *f(n) is Big O of g(n)*. \n\n&gt;O(f)={g\u2223g(n)\u2264Mf(n) for some M&gt;0 for large n}.\n\nOr f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\n&gt;Misconception 3: \u201cBig-O is a Statement About Time\u201d\n\nIt's confusing when books or articles use phrases such as \"The running time of &lt;algorithm&gt; is O(n log n)\". One thinks that Big O is about running time which is not technically correct as you said. \n\n&gt;Misconception 4: Big-O Is About Worst Case\n\nIn the example you gave, if someone says randomized Quicksort has complexity O(n log n) they are wrong, because that's average case. You can measure best case, average case and worst case analysis with Big O, but it still is a worst case complexity in all three cases. ", "aSentId": 43789, "answer": "&gt;Well, we're not comparing two functions though\n\nIf I say f(n) = O(g(n)), and f'(n) = O(g(n)), since equality is transitive, that should imply f(n) = f'(n).\n\n&gt;Or f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\nBig-O notation only cares about asymptotic bounds, so your definition is wrong. \"For large n\" is non-optional.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43788, "question": "I'm not sure I agree with misconception 1, please correct me if I'm wrong. \n\n&gt;is a widespread travestry. If you take it at face value, you can deduce that since 5n and 3n are both equal to O(n), then 3n must be equal to 5n and so 3=5.\n\nWell, we're not comparing two functions though, it's not f(n) = g(n) it's f(n) = O(g(n)) which is different. Furthermore by definition of big O we're ignoring multiplicative constants and lower order terms (i.e. it's asymptotic). I read f(n) = O(g(n)) as *f(n) is Big O of g(n)*. \n\n&gt;O(f)={g\u2223g(n)\u2264Mf(n) for some M&gt;0 for large n}.\n\nOr f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\n&gt;Misconception 3: \u201cBig-O is a Statement About Time\u201d\n\nIt's confusing when books or articles use phrases such as \"The running time of &lt;algorithm&gt; is O(n log n)\". One thinks that Big O is about running time which is not technically correct as you said. \n\n&gt;Misconception 4: Big-O Is About Worst Case\n\nIn the example you gave, if someone says randomized Quicksort has complexity O(n log n) they are wrong, because that's average case. You can measure best case, average case and worst case analysis with Big O, but it still is a worst case complexity in all three cases. ", "aSentId": 43791, "answer": "As for misconception 1, the point of the author is that equality, at the very least, refers to an equivalence relation that's (i) commutative: A = B if and only if B = A; (ii) transitive: A = B and B = C implies A = C; and (iii) reflexive: A = A for all A.  If you assume this to be true, you can legitimately derive outrageously false statements as the author has done.  The way you think about/read the statement as \"f(n) is Big-O of g(n)\" is correct, but that's not what equality says.  To express this statement with notation we shouldn't use equality or an equivalence relation because this isn't a symmetric/transitive relationship.  That's why element-of or subset (dependeing on the context) makes much more sense.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43788, "question": "I'm not sure I agree with misconception 1, please correct me if I'm wrong. \n\n&gt;is a widespread travestry. If you take it at face value, you can deduce that since 5n and 3n are both equal to O(n), then 3n must be equal to 5n and so 3=5.\n\nWell, we're not comparing two functions though, it's not f(n) = g(n) it's f(n) = O(g(n)) which is different. Furthermore by definition of big O we're ignoring multiplicative constants and lower order terms (i.e. it's asymptotic). I read f(n) = O(g(n)) as *f(n) is Big O of g(n)*. \n\n&gt;O(f)={g\u2223g(n)\u2264Mf(n) for some M&gt;0 for large n}.\n\nOr f(n) is O(g(n)) if f(n) &lt;= c * g(n). I feel this is more intuitive because you're thinking about Big O as being an upper bound.\n\n&gt;Misconception 3: \u201cBig-O is a Statement About Time\u201d\n\nIt's confusing when books or articles use phrases such as \"The running time of &lt;algorithm&gt; is O(n log n)\". One thinks that Big O is about running time which is not technically correct as you said. \n\n&gt;Misconception 4: Big-O Is About Worst Case\n\nIn the example you gave, if someone says randomized Quicksort has complexity O(n log n) they are wrong, because that's average case. You can measure best case, average case and worst case analysis with Big O, but it still is a worst case complexity in all three cases. ", "aSentId": 43793, "answer": "Regarding Misconception 4, when talking about randomized algorithms, it's not common to consider the worst/best/average cases.  As my undergrad classes taught me, QuickSort runs in expected Theta(n log(n)) steps.\n\nHowever, that's not really what's cool about QuickSort.  Instead, as a host of randomness in computing graduate courses explained to me, the probability that it will take more than Theta(n log(n)) steps is extremely low, and drops off exponentially as n grows.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43795, "question": "The article misses my biggest pet peeve about Big O notation: using *f(n) \u2208 O(g(n))* instead of *f \u2208 O(g)*.  *f* is a function, *f(n)* is not.", "aSentId": 43796, "answer": "*f(n)* is accepted mathematical notation for a function with one free variable. There's no problem here.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43795, "question": "The article misses my biggest pet peeve about Big O notation: using *f(n) \u2208 O(g(n))* instead of *f \u2208 O(g)*.  *f* is a function, *f(n)* is not.", "aSentId": 43798, "answer": "haha you must hate all the comments on this post", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43795, "question": "The article misses my biggest pet peeve about Big O notation: using *f(n) \u2208 O(g(n))* instead of *f \u2208 O(g)*.  *f* is a function, *f(n)* is not.", "aSentId": 43800, "answer": "So how would you write *f(n) \u2208 O(n log n)*? Are you required to write it in two lines, *g(n) = n log n; f \u2208 O(g)*?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43801, "question": "So how would you write *f(n) \u2208 O(n log n)*? Are you required to write it in two lines, *g(n) = n log n; f \u2208 O(g)*?", "aSentId": 43802, "answer": "*f \u2208 O(n \u21a6 n lg n)*", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43806, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 43807, "answer": "It's not unreasonable or uncommon to neglect the asymptotic cost of dealing with big numbers. We do this all the time when indexing arrays, for instance. You could argue that traversing from one array cell to the next is not O(1) because incrementing the index becomes more expensive as the number of digits in the index grows. (That darn carry from 999...999 to 1000...000.) Ultimately, we sacrifice some pedantry for the sake of simplicity, whether it's a uniform memory model or an arbitrarily big hash function.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43808, "question": "It's not unreasonable or uncommon to neglect the asymptotic cost of dealing with big numbers. We do this all the time when indexing arrays, for instance. You could argue that traversing from one array cell to the next is not O(1) because incrementing the index becomes more expensive as the number of digits in the index grows. (That darn carry from 999...999 to 1000...000.) Ultimately, we sacrifice some pedantry for the sake of simplicity, whether it's a uniform memory model or an arbitrarily big hash function.", "aSentId": 43809, "answer": "But a) the whole point of big-O is that we don't get to place bounds on problem size, and b) for that reason I have a hard time saying that memory access is constant either!\n\nSo (and I admit this is changing the question a bit), what if I'm in contexts where I'm judged by someone by-the-book and I also don't want to say anything I consider to be wrong?\n\n- \"It's constant across real-world ranges.\" ?\n- \"Within standard memory limits, it's constant.\" ?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43806, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 43811, "answer": "Usually, long before you hit the \"extremely high values\", you have either...\n\n* Run out of ram available to real world processes.\n* Or used up more CPU time than you have patience for.\n\nAn awful lot of \"O\" arithmetic quietly assumes \"infinite\" computers running finite problems, for the simple reason trying to fully account for the complexity of real computers is too hard.\n\nBy the time you get into cache effects and swapping and multiple cores and NUMA and.... Sigh! It's plain easier and more reliable to measure for realistic loads.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43806, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 43813, "answer": "Dude. Why is even array access considered `O(1)`? At least within our universe, as long as we don't find anything faster than light to use as a signal, it doesn't matter how efficiently we pack data on a 3d volume. It will still take at least `O(n^(1/3))` time for the signal to propagate from a point to another.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43815, "question": "If you are using a linked-list implementation of a HashTable, yes, lookup can be, in the worst case O(n). However, if you are using a statically sized HashTable with replacement (1 item per bucket), you will have O(1) for lookup. ", "aSentId": 43816, "answer": "But that would be a special case, not a general must-keep-all-keys hashtable.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43820, "question": "Advice about how to study CS", "aSentId": 43821, "answer": "My advice is to take classes that interest you and try and learn as much as you can while in school. You'll never be in a better place to learn than you are right now. If there's stuff that interests you right now, pursue it right now -- hopefully your school has ways to support you in that, and even if not you're in the right environment.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43823, "question": "Fixing a typo is probably the most rewarding and worst feeling ever.", "aSentId": 43824, "answer": "These bugs that take the longest to find tend to have the simplest and many times, stupidest, solutions.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43827, "question": "Looking to learn my first language -- any recommendations/course of action to learning it quickly?", "aSentId": 43828, "answer": "APL", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43827, "question": "Looking to learn my first language -- any recommendations/course of action to learning it quickly?", "aSentId": 43830, "answer": "Learn Haskell now. It's very mathematical, and even if it's not your main language, it will teach you to code in a way that improves the quality of code you write in other languages. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43831, "question": "Learn Haskell now. It's very mathematical, and even if it's not your main language, it will teach you to code in a way that improves the quality of code you write in other languages. ", "aSentId": 43832, "answer": "Maybe I'm naive. because I don't have a strong mathematical background and my first language was imperative, but I'd consider a purely functional language, like Haskell, to be intermediate level. Maybe beginner for someone with a strong mathematical grounding.\n\nAt least with functional languages that aren't pure you're free to learn in an imperative/OO way, even if that's not how the language was 'intended' to be used.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43834, "question": "Optimization of multiple attributes", "aSentId": 43835, "answer": "The isn't an optimization problem. You have no \"objective\" function that you're either maximizing or minimizing. You simply need to stable sort by attribute. The order of which attribute sorts will determine the \"weight\" assigned to each attribute.\n\nIf you need something to google, then google radix sort. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43837, "question": "Interesting look on those crazy interviews everyone talks about", "aSentId": 43838, "answer": "I still hear quite a bit about the shockingly large fraction of interviewees who can't solve the FizzBuzz problem. Anybody who's conducted interviews like this like to chime in? Is it mostly people who haven't held a software engineering job before?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43839, "question": "I still hear quite a bit about the shockingly large fraction of interviewees who can't solve the FizzBuzz problem. Anybody who's conducted interviews like this like to chime in? Is it mostly people who haven't held a software engineering job before?", "aSentId": 43840, "answer": "It's important to keep in mind that competent engineers who are easily performing in a stable role tend to not be the kind of people who are constantly going around applying for positions. Incompetent programmers make up a much greater percentage of the applicant pool than they do in pool of all programmers. Even if they held a job before, it's still going to be tougher for them to get a new one if their skills don't qualify them.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43842, "question": "A Theory of Changes for Higher-Order Languages [pdf]", "aSentId": 43843, "answer": "That indeed looks extremely cool and useful. Wouldn't this improve the complexity of functional reactive programs that re-render a tree every frame... or of just games with scenegraphs, for that matter?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43842, "question": "A Theory of Changes for Higher-Order Languages [pdf]", "aSentId": 43845, "answer": "Damn, I just started reading, this looks super cool.\n\nE: I'd love to use this in build systems.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43846, "question": "Damn, I just started reading, this looks super cool.\n\nE: I'd love to use this in build systems.", "aSentId": 43847, "answer": "We'll have to wait until they scale this technique up to handle languages with effects, which can be a long, hard road to travel", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43842, "question": "A Theory of Changes for Higher-Order Languages [pdf]", "aSentId": 43849, "answer": "Are the changes normalized? i.e if two changes results in same output, can we see know that the outputs are exactly the same?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43851, "question": "Is (was?) there a Carl Sagan/NDT of CS?", "aSentId": 43852, "answer": "Donald Knuth opened the doors to algorithms for many and still does ground breaking work. He writes voluminously and his work is very readable. We had books before there were YouTube videos.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43851, "question": "Is (was?) there a Carl Sagan/NDT of CS?", "aSentId": 43854, "answer": "No, nobody with anywhere close to the penetration of Sagan or NDT.  However, I really don't see a reason why there couldn't be . . . there should be.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43851, "question": "Is (was?) there a Carl Sagan/NDT of CS?", "aSentId": 43856, "answer": "My guess for a somewhat analogous popularizer or public advocate of computer science would be Grace Hopper. She didn't have nearly the reach or cultural exposure, but she did get out there as a touring lecturer later in her life. I remember she visited my middle school once, back in the 1980s.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43858, "question": "Steve Jobs\n\nedit: Sheldon Cooper", "aSentId": 43859, "answer": "Steve Jobs has contributed about as much to the field of computer science as an actor in a Viagra commercial has to the field of biomedical research.\n\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43860, "question": "Steve Jobs has contributed about as much to the field of computer science as an actor in a Viagra commercial has to the field of biomedical research.\n\n", "aSentId": 43861, "answer": "Which reminds one that when Steve Jobs died, the world went on a grief orgy that lasted for months; and when Dennis Ritchie died a week later, it barely made the papers. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43862, "question": "Which reminds one that when Steve Jobs died, the world went on a grief orgy that lasted for months; and when Dennis Ritchie died a week later, it barely made the papers. ", "aSentId": 43863, "answer": "That's the joke", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43860, "question": "Steve Jobs has contributed about as much to the field of computer science as an actor in a Viagra commercial has to the field of biomedical research.\n\n", "aSentId": 43865, "answer": "How much has Neil Degrasse Tyson done for astronomy?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 43866, "question": "How much has Neil Degrasse Tyson done for astronomy?", "aSentId": 43867, "answer": "He is an active educator in the field, and it helps to get people into astronomy when it's no longer some super academic and fancy field but something that can be understood rather easily. Making it accessible is more important than anything.\n\nMaybe you can argue that Steve Jobs has inspired people to go into technology, but IMO even Mark Zuckerberg and Google's founders are better at that than Steve Jobs. Or if you actually know Apple's history, then it's Steve Wozniak's influence which overshadows that of Steve Jobs. And nobody beats out Bill Gates in that department, anyway.\n\nBut what they all lack is the Carl Sagan educational+enthusiasm factor.", "corpus": "reddit"}]
