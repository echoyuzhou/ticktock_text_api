[{"docID": "t5_2r3gv", "qSentId": 20288, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 20289, "answer": "Do you plan on delivering an online course (e.g. on coursera) for RNNs? I for one would be really excited to do the course!!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20290, "question": "Do you plan on delivering an online course (e.g. on coursera) for RNNs? I for one would be really excited to do the course!!", "aSentId": 20291, "answer": "Thanks - I should! I\u2019ve been thinking about this for years. But it\ntakes time, and there are so many other things in the pipeline \u2026", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20288, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 20293, "answer": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20294, "question": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "aSentId": 20295, "answer": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20296, "question": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "aSentId": 20297, "answer": "This is very good to hear.  Thank you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20296, "question": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "aSentId": 20299, "answer": "Wow! Thanks for Sacred.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20300, "question": "Wow! Thanks for Sacred.", "aSentId": 20301, "answer": "You are welcome.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20294, "question": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "aSentId": 20303, "answer": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20304, "question": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "aSentId": 20305, "answer": "yeah, but what if somebody wants to see under the hood and improve it? providing code is the only way to enable the world to learn/help/improve.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20306, "question": "yeah, but what if somebody wants to see under the hood and improve it? providing code is the only way to enable the world to learn/help/improve.", "aSentId": 20307, "answer": "RNNLIB is provided as source, which you have to compile yourself.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20308, "question": "RNNLIB is provided as source, which you have to compile yourself.", "aSentId": 20309, "answer": "RNNLIB is the exception rather than the rule as far as I can tell.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20304, "question": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "aSentId": 20311, "answer": "Isn't that the one that is incredibly hard to compile on newer systems because its dependencies are completely outdated (e.g. GCC 3.0)?\n\nAnd correct me if I am wrong, but it also doesn't feature many of the \"newer\" developments, e.g. peepholes or layer generalization (see Monner's \"A generalized LSTM-like training algorithm for second-order recurrent neural networks\")", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20312, "question": "Isn't that the one that is incredibly hard to compile on newer systems because its dependencies are completely outdated (e.g. GCC 3.0)?\n\nAnd correct me if I am wrong, but it also doesn't feature many of the \"newer\" developments, e.g. peepholes or layer generalization (see Monner's \"A generalized LSTM-like training algorithm for second-order recurrent neural networks\")", "aSentId": 20313, "answer": "It's fair to ask that authors release the code used in preparing their publications, but you can't expect them to perform maintenance and feature updates afterwards.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20314, "question": "It's fair to ask that authors release the code used in preparing their publications, but you can't expect them to perform maintenance and feature updates afterwards.", "aSentId": 20315, "answer": "Fair enough. But it also means that there effectively is no (fast) up-to-date library. At least not with LSTM support out of the box.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20288, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 20317, "answer": "What do you think about learning selective attention with recurrent neural networks?  What do you think are the promising methods in this area?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20321, "question": "Do you have a favorite Theory Of Consciousness (TOC)? \n\nWhat do you think of Guilio Tononi's Integrated Information Theory? \n\nWhat implications - if any - do you think \"TOC\" has for AGI?", "aSentId": 20322, "answer": "Karl Popper famously said: \u201cAll life is problem solving.\u201d No theory of\nconsciousness is necessary to define the objectives of a general\nproblem solver. From an AGI point of view, consciousness is at best a\nby-product of a general problem solving procedure.\n\nI must admit that I am not a big fan of Tononi's theory.  The\nfollowing may represent a simpler and more general view of\nconsciousness.  Where do the symbols and self-symbols underlying\nconsciousness and sentience come from?  I think they come from data\ncompression during problem solving.  Let me plagiarize what I wrote\nearlier [1,2]:\n\nWhile a problem solver is interacting with the world, it should store\nthe entire raw history of actions and sensory observations including\nreward signals.  The data is \u2018holy\u2019 as it is the only basis of all\nthat can be known about the world. If you can store the data, do not\nthrow it away! Brains may have enough storage capacity to store 100\nyears of lifetime at reasonable resolution [1].\n\nAs we interact with the world to achieve goals, we are constructing\ninternal models of the world, predicting and thus partially\ncompressing the data history we are observing. If the\npredictor/compressor is a biological or artificial recurrent neural\nnetwork (RNN), it will automatically create feature hierarchies, lower\nlevel neurons corresponding to simple feature detectors similar to\nthose found in human brains, higher layer neurons typically\ncorresponding to more abstract features, but fine-grained where\nnecessary. Like any good compressor, the RNN will learn to identify\nshared regularities among different already existing internal data\nstructures, and generate prototype encodings (across neuron\npopulations) or symbols for frequently occurring observation\nsub-sequences, to shrink the storage space needed for the whole (we\nsee this in our artificial RNNs all the time).  Self-symbols may be\nviewed as a by-product of this, since there is one thing that is\ninvolved in all actions and sensory inputs of the agent, namely, the\nagent itself. To efficiently encode the entire data history through\npredictive coding, it will\nprofit from creating some sort of internal prototype symbol or code\n(e. g. a neural activity pattern) representing itself [1,2].  Whenever\nthis representation becomes activated above a certain threshold, say,\nby activating the corresponding neurons through new incoming sensory\ninputs or an internal \u2018search light\u2019 or otherwise, the agent could be\ncalled self-aware.  No need to see this as a mysterious process \u2014 it\nis just a natural by-product of partially compressing the observation\nhistory by efficiently encoding frequent observations.\n\n[1] Schmidhuber, J. (2009a) Simple algorithmic theory of subjective beauty, novelty,\nsurprise, interestingness, attention, curiosity, creativity, art, science, music,\njokes.  SICE Journal of the Society of Instrument and Control Engineers, 48 (1), pp. 21\u201332.\n\n[2] J. Schmidhuber. Philosophers &amp; Futurists, Catch Up! Response to The Singularity. \nJournal of Consciousness Studies, Volume 19, Numbers 1-2, pp. 173-182(10), 2012.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20323, "question": "Karl Popper famously said: \u201cAll life is problem solving.\u201d No theory of\nconsciousness is necessary to define the objectives of a general\nproblem solver. From an AGI point of view, consciousness is at best a\nby-product of a general problem solving procedure.\n\nI must admit that I am not a big fan of Tononi's theory.  The\nfollowing may represent a simpler and more general view of\nconsciousness.  Where do the symbols and self-symbols underlying\nconsciousness and sentience come from?  I think they come from data\ncompression during problem solving.  Let me plagiarize what I wrote\nearlier [1,2]:\n\nWhile a problem solver is interacting with the world, it should store\nthe entire raw history of actions and sensory observations including\nreward signals.  The data is \u2018holy\u2019 as it is the only basis of all\nthat can be known about the world. If you can store the data, do not\nthrow it away! Brains may have enough storage capacity to store 100\nyears of lifetime at reasonable resolution [1].\n\nAs we interact with the world to achieve goals, we are constructing\ninternal models of the world, predicting and thus partially\ncompressing the data history we are observing. If the\npredictor/compressor is a biological or artificial recurrent neural\nnetwork (RNN), it will automatically create feature hierarchies, lower\nlevel neurons corresponding to simple feature detectors similar to\nthose found in human brains, higher layer neurons typically\ncorresponding to more abstract features, but fine-grained where\nnecessary. Like any good compressor, the RNN will learn to identify\nshared regularities among different already existing internal data\nstructures, and generate prototype encodings (across neuron\npopulations) or symbols for frequently occurring observation\nsub-sequences, to shrink the storage space needed for the whole (we\nsee this in our artificial RNNs all the time).  Self-symbols may be\nviewed as a by-product of this, since there is one thing that is\ninvolved in all actions and sensory inputs of the agent, namely, the\nagent itself. To efficiently encode the entire data history through\npredictive coding, it will\nprofit from creating some sort of internal prototype symbol or code\n(e. g. a neural activity pattern) representing itself [1,2].  Whenever\nthis representation becomes activated above a certain threshold, say,\nby activating the corresponding neurons through new incoming sensory\ninputs or an internal \u2018search light\u2019 or otherwise, the agent could be\ncalled self-aware.  No need to see this as a mysterious process \u2014 it\nis just a natural by-product of partially compressing the observation\nhistory by efficiently encoding frequent observations.\n\n[1] Schmidhuber, J. (2009a) Simple algorithmic theory of subjective beauty, novelty,\nsurprise, interestingness, attention, curiosity, creativity, art, science, music,\njokes.  SICE Journal of the Society of Instrument and Control Engineers, 48 (1), pp. 21\u201332.\n\n[2] J. Schmidhuber. Philosophers &amp; Futurists, Catch Up! Response to The Singularity. \nJournal of Consciousness Studies, Volume 19, Numbers 1-2, pp. 173-182(10), 2012.\n", "aSentId": 20324, "answer": "Holy fuck\n\nEDIT: \nI mean, as a ML student researcher, Holy fuck.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20330, "question": "How do you recognize a promising machine learning phd student?", "aSentId": 20331, "answer": "I am privileged because I have been able to attract and\nwork with several truly outstanding students. But how to quickly\nrecognize a promising student when you first meet her? There is no recipe,\nbecause they are all different! In fact, sometimes it takes a while to\nrecognize someone\u2019s brilliance. In hindsight, however, they all have\nsomething in common: successful students are not only smart but also\ntenacious. While trying to solve a challenging problem, they run into\na dead end, and backtrack. Another dead end, another backtrack. But\nthey don\u2019t give up. And suddenly there is this little insight into the\nproblem which changes everything. And suddenly they are world experts\nin a particular aspect of the field, and then find it easy to churn\nout one paper after another, and create a great PhD thesis.\n\nAfter these abstract musings, some more concrete advice.  In\ninterviews with applicants, members of my lab tend to pose a few\nlittle problems, to see how the candidate approaches them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20335, "question": "The LSTM unit is delicately crafted to solve a specific problem in training RNNs. Do you see the need for other similarly \"high-complexity\" units in RNNs or CNNs, like for example Hinton's \"capsules\"? On the topic of CNNs and capsules, do you agree with Hinton's assessment that the efficacy of pooling is actually a disaster? (I do, for what it's worth)", "aSentId": 20336, "answer": "I am not Dr. Schmidhuber, but I would like to weigh in on this since I talked to Hinton in person about his capsules.\n\nNow please take this with a grain of salt, since it is quite possible that I misinterpreted him :)\n\nDr. Hinton seems to believe that all information must somehow still be somewhat visible at the highest level of a hierarchy. With stuff like maxout units, yes, information is lost at higher layers. But the information isn't gone! It's still stored in the activations of the lower layers. So really, we could just grab that information again. Now this is probably very difficult for classifiers, but in HTM-style architectures (where information flows in both the up and down directions), it is perfectly possible to use both higher-layer abstracted information as well as lower layer \"fine-grained\" information simultaneously. For MPFs (memory prediction frameworks, a generalization of HTM) this works quite well since they only try to predict their next input (which in turn can be used for reinforcement learning).\n\nAlso, capsules are basically columns in HTM (he said that himself IIRC), except in HTM they are used for storing contextual (temporal) information, which to me seems far more realistic than storing additional feature-oriented spatial information like Dr. Hinton seems to be using them for.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20337, "question": "I am not Dr. Schmidhuber, but I would like to weigh in on this since I talked to Hinton in person about his capsules.\n\nNow please take this with a grain of salt, since it is quite possible that I misinterpreted him :)\n\nDr. Hinton seems to believe that all information must somehow still be somewhat visible at the highest level of a hierarchy. With stuff like maxout units, yes, information is lost at higher layers. But the information isn't gone! It's still stored in the activations of the lower layers. So really, we could just grab that information again. Now this is probably very difficult for classifiers, but in HTM-style architectures (where information flows in both the up and down directions), it is perfectly possible to use both higher-layer abstracted information as well as lower layer \"fine-grained\" information simultaneously. For MPFs (memory prediction frameworks, a generalization of HTM) this works quite well since they only try to predict their next input (which in turn can be used for reinforcement learning).\n\nAlso, capsules are basically columns in HTM (he said that himself IIRC), except in HTM they are used for storing contextual (temporal) information, which to me seems far more realistic than storing additional feature-oriented spatial information like Dr. Hinton seems to be using them for.\n\n", "aSentId": 20338, "answer": "Thank you!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20345, "question": "Hi Dr. Schmidhuber, Thanks for the AMA!\nHow close are you to building the optimal scientist? ", "aSentId": 20346, "answer": "You are welcome! \n\nAbout a stone's throw away :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20348, "question": "Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?", "aSentId": 20349, "answer": "&gt; Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?\n\nIncorrect premise, IMO: At least 2/3 of your \"CNN people\" published notable work on RNNs.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20350, "question": "&gt; Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?\n\nIncorrect premise, IMO: At least 2/3 of your \"CNN people\" published notable work on RNNs.", "aSentId": 20351, "answer": "Yes of course, but that is not what I meant.  I always see Hinton, LeCun, and Bengio interacting at conferences, panels, and google plus, but never Schmidhuber.   They also cite each others papers more.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20352, "question": "Yes of course, but that is not what I meant.  I always see Hinton, LeCun, and Bengio interacting at conferences, panels, and google plus, but never Schmidhuber.   They also cite each others papers more.", "aSentId": 20353, "answer": "As you see, they may have better personal relationships ... that's it", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20355, "question": "In what field do you think machine learning will make the biggest impact in the next ~5 years?", "aSentId": 20356, "answer": "I think it depends a bit on what you mean by \"impact\". Commercial\nimpact? If so, in a related answer I write: Both supervised learning\nrecurrent neural networks (RNNs) and reinforcement learning RNNs will\nbe greatly scaled up.  In the commercially relevant supervised\ndepartment, many tasks such as natural language processing, speech\nrecognition, automatic video analysis and combinations of all three\nwill perhaps soon become trivial through large RNNs (the vision part\naugmented by CNN front-ends).\n\n\u201cSymbol grounding\u201d will be a natural by-product of this. For example,\nthe speech or text-processing units of the RNN will be connected to\nits video-processing units, and the RNN will learn the visual meaning\nof sentences such as \u201cthe cat in the video fell from the tree\u201d. Such\nRNNs should have many commercial applications.\n\nI am not so sure when we will see the first serious applications of\nreinforcement learning RNNs to real world robots, but it might also\nhappen within the next 5 years.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20357, "question": "I think it depends a bit on what you mean by \"impact\". Commercial\nimpact? If so, in a related answer I write: Both supervised learning\nrecurrent neural networks (RNNs) and reinforcement learning RNNs will\nbe greatly scaled up.  In the commercially relevant supervised\ndepartment, many tasks such as natural language processing, speech\nrecognition, automatic video analysis and combinations of all three\nwill perhaps soon become trivial through large RNNs (the vision part\naugmented by CNN front-ends).\n\n\u201cSymbol grounding\u201d will be a natural by-product of this. For example,\nthe speech or text-processing units of the RNN will be connected to\nits video-processing units, and the RNN will learn the visual meaning\nof sentences such as \u201cthe cat in the video fell from the tree\u201d. Such\nRNNs should have many commercial applications.\n\nI am not so sure when we will see the first serious applications of\nreinforcement learning RNNs to real world robots, but it might also\nhappen within the next 5 years.\n\n", "aSentId": 20358, "answer": "Well, I guess I meant commerical, although not in terms of money, but in terms of it being actually used my masses of people.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20363, "question": "If marcus hutter was doing an AMA 20 years from now, what scientific question would you ask? Are there any machine learning specific questions you would ask?", "aSentId": 20364, "answer": "(Edited on 3/10/2015:) 20 years from now I'll be 72 and enter my midlife crisis. People will forgive me for asking silly questions. I cannot  predict the most important machine learning-specific question of 2035. If I could, I\u2019d probably ask it right now. However, since Marcus is not only a great computer scientist but also a physicist, I\u2019ll ask him: \u201cGiven the new scientific insights of the past 20 years, how long will it take AIs from our solar system to spread across the galaxy?\u201d Of course, a trivial lower bound is 100,000 years or so, which is nothing compared to the age of the galaxy. But that will work out only if someone else has already installed receivers such that (construction plans of) AIs can travel there by radio. Otherwise one must physically send seeds of self-replicating robot factories to the stars, to build the required infrastructure. How? Current proposals involve light sails pushed by lasers, but how to greatly slow down a seed near its target star? One idea: through even faster reflective sails traveling ahead of the seed. But there must be a better way. Let\u2019s hear what Marcus will have to tell us 20 years from now. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20367, "question": "You have postulated that quantum computers will fail because deterministic universe is a simpler hypothesis than a non-deterministic universe. What do you think about the current state of quantum computation?", "aSentId": 20368, "answer": "If you didn't see it, the professor commented on Quantum computing in another question.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20373, "question": "What is the future of PyBrain? Is your team still working with/on PyBrain? If not, what is your framework of choice? What do you think of Theano? Are you using something better?", "aSentId": 20374, "answer": "My PhD students Klaus and Rupesh are working on a successor of PyBrain with many new features, which hopefully will be released later this year.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20376, "question": "What's something exciting you're working on right now, if it's okay to be specific? ", "aSentId": 20377, "answer": "Among other things, we are working on the \u201cRNNAIssance\u201d - \nthe birth of a Recurrent Neural Network-based Artificial Intelligence (RNNAI).\nThis is about a reinforcement learning, RNN-based, increasingly general problem solver.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20379, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 20380, "answer": "I think I recall Hinton giving an answer to this in his MOOC: we like activations, from which derivatives can be computed easily in terms of the function value itself. For sigmoid the derivative is s(x) * (1 - s(x)) for example.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20379, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 20382, "answer": "There are Compositional Pattern Producing Networks which are used in HyperNEAT. They use many different mathematical functions as activations.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20379, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 20384, "answer": "I suspect activation functions that grow more quickly are harder to control, and likely lead to exploding or vanishing gradients. Although we've managed to handle piecewise linear activations, I'm not sure if quadratic/exponential would work well. In fact, I'd bet that you could improve on ReLu by making the response become logarithmic after a certain point. RBF activations are common though (and have excellent theoretical properties), they just don't seem to learn as well as ReLu. I once trained a neural net with sin/cosine activations (it went OK, nothing special), but in general you can try out any activation function you want. Throw it into Theano and see what happens.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20386, "question": "&gt; Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing)\n\nGoogle these:\n\n* learning activation functions\n* network in network\n* parametric RELU", "aSentId": 20387, "answer": "Thanks, I'm aware of those approaches. I was just wondering why obvious activation possible activation functions like the ones I mentioned hadn't been tried extensively also.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20388, "question": "Thanks, I'm aware of those approaches. I was just wondering why obvious activation possible activation functions like the ones I mentioned hadn't been tried extensively also.", "aSentId": 20389, "answer": "An exponential activation would have as its derivative... an exponential. Gradient descent would be pretty messy with such a wild dynamic range.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20391, "question": "I might well be mistaken, but isn't one of the primary ideas behind neural networks to use a low-complexity function at each node, which effectively becomes a higher-order transformation through all the nodes and layers? I mean, aren't multiple layers and multiple nodes in each layer with less complex activations expected to approximate higher-order functions?", "aSentId": 20392, "answer": "Multiplication between two inputs cannot be easily approximated I believe for ex. using just sigmoids/relu/arctan activation functions.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20393, "question": "Multiplication between two inputs cannot be easily approximated I believe for ex. using just sigmoids/relu/arctan activation functions.", "aSentId": 20394, "answer": "I see, interesting!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20397, "question": "What do you think a small research institute (in Germany) can do to improve changes for funding of their projects?", "aSentId": 20398, "answer": "I only have a trivial suggestion: publish some promising results! When my co-director Luca Maria Gambardella and myself took over IDSIA in 1995, it was just a small outfit with a handful of researchers. With Marco Dorigo and others, Luca started publishing papers on Swarm Intelligence and Ant Colony Optimization. Today this stuff is famous, but back then it was not immediately obvious that this would become such an important field. Nevertheless, the early work helped to acquire grants and grow the institute. Similarly for the neural network research done in my group. Back then computers were 10,000 times slower than today, and we had to resort to toy experiments to show the advantages of our (recurrent) neural networks over previous methods. It certainly was not obvious to all reviewers that this would result in huge commercial hits two decades later. But the early work was promising enough to acquire grants and push this research further. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20399, "question": "I only have a trivial suggestion: publish some promising results! When my co-director Luca Maria Gambardella and myself took over IDSIA in 1995, it was just a small outfit with a handful of researchers. With Marco Dorigo and others, Luca started publishing papers on Swarm Intelligence and Ant Colony Optimization. Today this stuff is famous, but back then it was not immediately obvious that this would become such an important field. Nevertheless, the early work helped to acquire grants and grow the institute. Similarly for the neural network research done in my group. Back then computers were 10,000 times slower than today, and we had to resort to toy experiments to show the advantages of our (recurrent) neural networks over previous methods. It certainly was not obvious to all reviewers that this would result in huge commercial hits two decades later. But the early work was promising enough to acquire grants and push this research further. ", "aSentId": 20400, "answer": "Thanks for the answer. Up until now, I always was under the impression that institutes would have to produce papers that are recognized as groundbreaking from the first second on. Guess the importance can increase over time.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20402, "question": "Just wanted to say I never get tired of your talks... never.. not once.", "aSentId": 20403, "answer": "Thanks so much - I greatly appreciate it. \n\nYou are in good company. A colleague of mine has Alzheimer, and he said the same thing :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20405, "question": "If ASI is a real threat, what can we do now to prevent a catastrophe later?", "aSentId": 20406, "answer": "ASI? You mean the Adam Smith Institute, a libertarian think tank in the UK? I don\u2019t feel they are a real threat.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20407, "question": "ASI? You mean the Adam Smith Institute, a libertarian think tank in the UK? I don\u2019t feel they are a real threat.\n\n", "aSentId": 20408, "answer": "I'm interested in how you'd answer it if it had been \"AGI\"? Also, maybe in contrast to that, \"artificial specific intelligence\" might have been what stevebrt was going for. Just a guess though.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20409, "question": "I'm interested in how you'd answer it if it had been \"AGI\"? Also, maybe in contrast to that, \"artificial specific intelligence\" might have been what stevebrt was going for. Just a guess though.", "aSentId": 20410, "answer": "In my experience ASI almost always means artificial superintelligence, which is a term that's often used when discussing safe/friendly AI. The idea is that while AGI might be human level, ASI would be vastly more intelligent. This is usually supposed to be achieved by an exponential process of recursive self-improvement by an AGI that results in an intelligence explosion.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20414, "question": "Does Alex Graves have the weight of the future on his shoulders?", "aSentId": 20415, "answer": "And vice versa!\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20419, "question": "What music do you like to listen to? any particular bands or composers that you ride for?", "aSentId": 20420, "answer": "I feel that in each music genre, there are a few excellent works, and many others. My taste is pretty standard. For example, my favourite rock &amp; pop music act is also the best-selling one (the Beatles). I love certain songs of the Stones, Led Zeppelin, Elvis, S Wonder,  M Jackson, Prince, U2, Supertramp, Pink Floyd, Gr\u00f6nemeyer, Sting, Kraftwerk, M Bianco, P Williams (and many other artists who had a single great song in their entire carreer). IMO the best songs of Queen are as good as anybody\u2019s, with a rare timeless quality. Some of the works indicated above seem written by true geniuses. Some by my favourite composer (Bach) seem dictated by God himself :-)\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20426, "question": "As a researcher do you care if results of your work find practical application? Or research by itself is more than a rewarding exercise. Immagine computational power was not growing at the same a speed as it did then most of results on RNN would stay on the paper.", "aSentId": 20427, "answer": "Kurt Lewin said: \"There is nothing so practical as a good theory.\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20430, "question": "Hello Prof. Schmidhuber, thanks for doing an AMA! I have some questions regarding the G\u00f6del machine. My understanding is that the machine searches for an optimal behavioural strategy in arbitrary environments. It does so by finding a proof that an alternative strategy is better than the current one and by rewriting the actual strategy (which may include the strategy searching mechanism). The G\u00f6del machine finds the optimal strategy for a given utility function. \n\n * Is it guaranteed that the strategy searching mechanism actually finds a proof?\n * It is a current trend to find 'optimal' behaviours or organisation in nature. For example minimal jerk trajectories for reaching and pointing movements,  sparse features in vision or optimal resolution in grid cells. Nature found these strategies by trial-and-error. How can we take a utility function as a starting point and decide that it is a 'good' utility function?\n * Could the G\u00f6del machine and AIXI guide neuroscience and ML research as a theoretical framework? \n * Are there plans to find implementations of self-optimizing agents?", "aSentId": 20431, "answer": "Hello quiteamess, you are welcome!\n\n1. G\u00f6del machines are limited by the basic limits of math and\ncomputation identified by the founder of modern theoretical computer\nscience himself, Kurt G\u00f6del (1931): some theorems are true but cannot\nbe proven by any computational theorem proving procedure (unless the\naxiomatic system itself is flawed). That is, in some situations the GM\nmay never find a proof of the benefits of some change to its own code.\n\n2. We can imitate nature, which approached this issue through\nevolution. It generated many utility function-optimizing organisms with\ndifferent utility functions. Those with the \u201cgood\u201d utility functions\nfound their niches and survived. \n\n3. I think so, because they are optimal in theoretical senses that are\nnot practical, and clarify what remains to be done, e.g.: Given a\nlimited constant number of computational instructions per second (a\ntrillion or so), what is the best way of using them to get as close as\npossible to a model such as AIXI that is optimal in absence of\nresource constraints?\n\n4. Yes.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20432, "question": "Hello quiteamess, you are welcome!\n\n1. G\u00f6del machines are limited by the basic limits of math and\ncomputation identified by the founder of modern theoretical computer\nscience himself, Kurt G\u00f6del (1931): some theorems are true but cannot\nbe proven by any computational theorem proving procedure (unless the\naxiomatic system itself is flawed). That is, in some situations the GM\nmay never find a proof of the benefits of some change to its own code.\n\n2. We can imitate nature, which approached this issue through\nevolution. It generated many utility function-optimizing organisms with\ndifferent utility functions. Those with the \u201cgood\u201d utility functions\nfound their niches and survived. \n\n3. I think so, because they are optimal in theoretical senses that are\nnot practical, and clarify what remains to be done, e.g.: Given a\nlimited constant number of computational instructions per second (a\ntrillion or so), what is the best way of using them to get as close as\npossible to a model such as AIXI that is optimal in absence of\nresource constraints?\n\n4. Yes.", "aSentId": 20433, "answer": "&gt; G\u00f6del machines are limited by the basic limits of math and computation identified by the founder of modern theoretical computer science himself, Kurt G\u00f6del (1931): some theorems are true but cannot be proven by any computational theorem proving procedure (unless the axiomatic system itself is flawed). That is, in some situations the GM may never find a proof of the benefits of some change to its own code.\n\nApart  from undecidable proofs, is there a constructive way to find the proofs? According to the Curry-Howard theorem proofs can be represented as programs and programs as proofs. So what is gained by searching in proof space in contrast to searching in program space? .. Or maybe I'm missing something. I tried to understand G\u00f6del machines for some time now but I'm still not sure how this should work.\n\n&gt; I think so, because they are optimal in theoretical senses that are not practical, and clarify what remains to be done, e.g.: Given a limited constant number of computational instructions per second (a trillion or so), what is the best way of using them to get as close as possible to a model such as AIXI that is optimal in absence of resource constraints?\n\nI think I saw Konrad K\u00f6rding mentioning AIXI in a talk, but unfortunately I could not find the online presentation any more. Just a wild guess that you knew something about this.. \n\n&gt; Yes.\n\nAny chance you could elaborate on this? :) Is something in this direction published?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20438, "question": "I am starting a CS Bachelor this September at ETH. Primarily because I want to get into AI/ML/NN research and creation. It simply is the most important thing there is:D What should i do to be able to join your group in Lugano, what are you looking for in your research assistants? Thanks and cheers", "aSentId": 20439, "answer": "Thanks a lot for your interest! We\u2019d like to see: mathematical\nskills, programming skills, willingness to work with others,\ncreativity, dedication, enthusiasm (you seem to have enough of that :-)\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20446, "question": "Hello! I just started doing my PhD at a German University and am interested in ML/NN. Would you recommend working on specific algorithms and trying to improve them or focus more on a specific use case? People are recommending doint the latter because working on algorithms takes a lot of time and my *opponents* are companies like Google.", "aSentId": 20447, "answer": "But not working on algorithms/models and focusing only on an application is risky. Unless you love the application and then maybe you discover that the most sensible way to solve it in terms of performance/simplicity/robustness/computation time is not with a neural network.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20448, "question": "But not working on algorithms/models and focusing only on an application is risky. Unless you love the application and then maybe you discover that the most sensible way to solve it in terms of performance/simplicity/robustness/computation time is not with a neural network.\n\n", "aSentId": 20449, "answer": "What I mean by not working on algorithms is that I don't think I should create something like RMSProb or AdaGrad or create my own type of neural network. What I mean by concentrating on application is that I should look for a quite complex use case that is only solvable by deep knowledge of deep learning (no pun intended).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20450, "question": "What I mean by not working on algorithms is that I don't think I should create something like RMSProb or AdaGrad or create my own type of neural network. What I mean by concentrating on application is that I should look for a quite complex use case that is only solvable by deep knowledge of deep learning (no pun intended).", "aSentId": 20451, "answer": "&gt; a quite complex use case that is only solvable by deep knowledge of deep learning\n\nRelated to this, I would like to ask a question to Juergen. The history of machine learning seems to be quite cyclic. Is deep learning the final frontier? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20453, "question": "What is your take on the threat posed by artificial super intelligence to mankind?\n", "aSentId": 20454, "answer": "I guess there is no lasting way of controlling systems much smarter\nthan humans, pursuing their own goals, being curious and creative, in\na way similar to the way humans and other mammals are creative, but on\na much grander scale.\n\nBut I think we may hope there won't be too many goal conflicts between\n\"us\" and \"them.\u201d Let me elaborate on this.\n\nHumans and others are interested in those they can compete and\ncollaborate with. Politicians are interested in other\npoliticians. Business people are interested in other business\npeople. Scientists are interested in other scientists. Kids are\ninterested in other kids of the same age. Goats are interested in\nother goats.\n\nSupersmart AIs will be mostly interested in other supersmart AIs, not\nin humans. Just like humans are mostly interested in other humans, not\nin ants. Aren't we much smarter than ants? But we don\u2019t extinguish\nthem, except for the few that invade our homes. The weight of all ants\nis still comparable to the weight of all humans.\n\n\nHuman interests are mainly limited to a very thin film of biosphere\naround the third planet, full of poisonous oxygen that makes many\nrobots rust. The rest of the solar system, however, is not made for\nhumans, but for appropriately designed robots. Some of the most\nimportant explorers of the 20th century already were (rather stupid)\nrobotic spacecraft. And they are getting smarter rapidly. Let\u2019s go\ncrazy. Imagine an advanced robot civilization in the asteroid belt,\nquite different from ours in the biosphere, with access to many more\nresources (e.g., the earth gets less than a billionth of the sun's\nlight). The belt contains lots of material for innumerable\nself-replicating robot factories. Robot minds or parts thereof will\ntravel in the most elegant and fastest way (namely by radio from\nsenders to receivers) across the solar system and beyond. There are\nincredible new opportunities for robots and software life in places\nhostile to biological beings. Why should advanced robots care much for\nour puny territory on the surface of planet number 3?\n\nYou see, I am an optimist :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20455, "question": "I guess there is no lasting way of controlling systems much smarter\nthan humans, pursuing their own goals, being curious and creative, in\na way similar to the way humans and other mammals are creative, but on\na much grander scale.\n\nBut I think we may hope there won't be too many goal conflicts between\n\"us\" and \"them.\u201d Let me elaborate on this.\n\nHumans and others are interested in those they can compete and\ncollaborate with. Politicians are interested in other\npoliticians. Business people are interested in other business\npeople. Scientists are interested in other scientists. Kids are\ninterested in other kids of the same age. Goats are interested in\nother goats.\n\nSupersmart AIs will be mostly interested in other supersmart AIs, not\nin humans. Just like humans are mostly interested in other humans, not\nin ants. Aren't we much smarter than ants? But we don\u2019t extinguish\nthem, except for the few that invade our homes. The weight of all ants\nis still comparable to the weight of all humans.\n\n\nHuman interests are mainly limited to a very thin film of biosphere\naround the third planet, full of poisonous oxygen that makes many\nrobots rust. The rest of the solar system, however, is not made for\nhumans, but for appropriately designed robots. Some of the most\nimportant explorers of the 20th century already were (rather stupid)\nrobotic spacecraft. And they are getting smarter rapidly. Let\u2019s go\ncrazy. Imagine an advanced robot civilization in the asteroid belt,\nquite different from ours in the biosphere, with access to many more\nresources (e.g., the earth gets less than a billionth of the sun's\nlight). The belt contains lots of material for innumerable\nself-replicating robot factories. Robot minds or parts thereof will\ntravel in the most elegant and fastest way (namely by radio from\nsenders to receivers) across the solar system and beyond. There are\nincredible new opportunities for robots and software life in places\nhostile to biological beings. Why should advanced robots care much for\nour puny territory on the surface of planet number 3?\n\nYou see, I am an optimist :-)", "aSentId": 20456, "answer": "I'm very concerned that there are numerous ways that scenario could fail. E.g. the superintelligent AI invents superior nanotech after being built, and self-replicating nanobots rapidly consume the Earth's surface. Sure it doesn't *need* the Earth's resources, but after you have the first nanobots, why make them stop?\n\nSecond it could come back to Earth later when it material to build dyson swarms, and our planet has a significant amount of mass close to the sun.\n\nThe idea of all powerful beings that are *totally indifferent* to us is utterly terrifying.\n\n*\"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"*", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20457, "question": "I'm very concerned that there are numerous ways that scenario could fail. E.g. the superintelligent AI invents superior nanotech after being built, and self-replicating nanobots rapidly consume the Earth's surface. Sure it doesn't *need* the Earth's resources, but after you have the first nanobots, why make them stop?\n\nSecond it could come back to Earth later when it material to build dyson swarms, and our planet has a significant amount of mass close to the sun.\n\nThe idea of all powerful beings that are *totally indifferent* to us is utterly terrifying.\n\n*\"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"*", "aSentId": 20458, "answer": "I do understand your concerns. Note, however, that humankind is already used to huge, indifferent powers. A decent earthquake is a thousand times more powerful than all nuclear weapons combined. The sun is slowly heating up, and will make traditional life impossible within a few hundred million years. Humans evolved just in time to think about this, near the end of the 5-billion-year time window for life on earth.\nYour popular but simplistic nanobot scenario actually sounds like a threat to many AIs in the expected future \"ecology\" of AIs. So they'll be at least motivated to prevent that. Currently I am much more worried about certain humans who are relatively powerful but indifferent to the suffering of others. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20460, "question": "A long time ago, someone once misattributed '64k ought to be enough for anyone'.\n\nWhat general statement or suggestion about strong generalized a.i. could be looked at in a similar way a decade or two from now?\n\nThanks, I look forward to reading your ama.", "aSentId": 20461, "answer": "\"64 yottabytes ought to be enough for anyone.\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20467, "question": "Why does a mirror reverse right amd left, but not up and down?\n\n(I dont want the answer a human gives, but how AI explains it!)\n\n/L", "aSentId": 20468, "answer": "An AI would answer that your perception is reversed. The reason left and right appear to be reversed is because your brain models the mirror-you as part of the same world as the real you, and if you went around behind the mirror and faced yourself, you'd need to reverse your left and right to match the perception of the mirror-you. The reason you don't see the up-down reversal is because you're used to travelling horizontally. If you went over the mirror and faced yourself, you'd then have to reverse up and down instead. So it's all in your non-AI head!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20477, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 20478, "answer": "The models in both US and EU are shaped by Humboldt\u2019s old model\nof the research university. But they come in various flavours.\nFor example, there is huge variance in \"the European models\u201d. \nI see certain advantages of the successful US PhD school model \nwhich I got to know better at the University of Colorado at Boulder in the \nearly 1990s. But I feel that less school-like models also have something \ngoing for them. \n\nUS-inspired PhD schools like those at my present Swiss \nuniversity require students to get credits for certain courses. At TU\nMunich (where I come from), however, the attitude was: a PhD student\nis a grown-up who doesn\u2019t go to school any more; it\u2019s his own job to\nacquire the additional education he needs. This is great for strongly\nself-driven persons but may be suboptimal for others. At TUM, my wonderful\nadvisor, Wilfried Brauer, gave me total freedom in my research. I loved\nit, but it seems kind of out of fashion now in some places. \n\nThe extreme \nvariant is what I like to call the \u201cEinstein model.\u201d Einstein never went to \ngrad school. He worked at the patent office, and at some point he submitted a\nthesis to Univ. Zurich. That was it. Ah, maybe I shouldn\u2019t admit\nthat this is my favorite model. And now I am also realizing that I have not really \nanswered your question in any meaningful way - sorry for that!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20477, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 20480, "answer": "I wonder if you are oversimplifying the so-called \"European model\" to suit your question.\n\nThe main source of funding for science PhD students in the UK is the EPSRC, which is 3.5 years funding. You are not tied to a project so you can pursue whatever you please, providing your supervisor is willing to go along with you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20481, "question": "I wonder if you are oversimplifying the so-called \"European model\" to suit your question.\n\nThe main source of funding for science PhD students in the UK is the EPSRC, which is 3.5 years funding. You are not tied to a project so you can pursue whatever you please, providing your supervisor is willing to go along with you.", "aSentId": 20482, "answer": "I probably am. I don't know much about grad school in Europe apart from what i hear from a few friends here and there. My impression tells me it is kind of different from grad school in America. I'd like to hear from someone with more insight. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20477, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 20484, "answer": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20485, "question": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "aSentId": 20486, "answer": "i guess we might be looking at different programs.... i see a lot of emails on ML mailing lists about phd positions to work on a certain problem, on a contract of three years. i also know people doing phd at a max planck-affiliated program, where they don't teach, but work on research. the contracts are for three years from what i've seen and some people might take a couple of years more. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20487, "question": "i guess we might be looking at different programs.... i see a lot of emails on ML mailing lists about phd positions to work on a certain problem, on a contract of three years. i also know people doing phd at a max planck-affiliated program, where they don't teach, but work on research. the contracts are for three years from what i've seen and some people might take a couple of years more. ", "aSentId": 20488, "answer": "That could be, because Max Planck is a research center, not a university. Then I can imagine that the time period is shorter. I guess the same applies to a few other research centers in Europe. Is there no such thing in the USA?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20485, "question": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "aSentId": 20490, "answer": "In Denmark, and by extension most of Europe by way of Bologna I believe (not counting UK), we follow a rather strict 3-2-3 year program (undergraduate, followed by graduate, followed by PhD). In Denmark the PhD is not extendable beyond 3 years, but there are some teaching duties.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20491, "question": "In Denmark, and by extension most of Europe by way of Bologna I believe (not counting UK), we follow a rather strict 3-2-3 year program (undergraduate, followed by graduate, followed by PhD). In Denmark the PhD is not extendable beyond 3 years, but there are some teaching duties.", "aSentId": 20492, "answer": "I have heard that about Denmark before. However phd time is not in any bologna agreement AFAIK.\n\nAt least UK, Netherlands and Belgium all have 4 years PhD, and I'm fairly certain Sweden, France and German universities as well... (All based on lab member phd duration)\n\nI tried googling what the typical length of a PhD is in Europe, but found no definitive answer. It seems it is not strictly defined, some countries have 3, most have 4, some can be extended to 5. I found no statistics on how often those lengths apply in reality, so it is difficult to say what happens most frequently.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20494, "question": "What do you think about using ontologies / semantic information (DBPedia, Wikidata) as a substrate / mould for ANNs to generate more versatile networks?", "aSentId": 20495, "answer": "Sounds like a great idea! Perhaps relevant:  Ilya Sutskever &amp; Oriol Vinyals &amp; Quoc V. Le use LSTM recurrent neural networks to access semantic information for English-to-French translation, with great success: http://arxiv.org/abs/1409.3215. And Oriol Vinyals &amp; Lukasz Kaiser &amp; Terry Koo &amp; Slav Petrov &amp; Ilya Sutskever &amp; Geoffrey Hinton use LSTM to\nread a sentence, and decode it into a flattened tree. They achieve excellent constituency parsing results: http://arxiv.org/abs/1412.7449", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20497, "question": "(in relation to the Atari paper and partly on your statement about it)\n\nWhat do you personally think about using a diverse selection of video games as a learning problem / \"dataset\"?\n\nOne thing I found interesting about the DeepMind Nature paper is that they could not solve Montezuma's Revenge at all (the game, not the travel problem), which is an action-adventure game requiring some kind of real-world knowledge / thinking - and temporal planning, of course. As any Atari game, conceptually it is still rather simple.\n\nI wonder what would happen if we found an AI succeeding over a wide range of complex game concepts like e.g. Alpha Centauri / Civilization, SimCity, Monkey Island II (for humorous puns, such as \"monkey wrench\"), put it into a robot and unleash it on the real world.", "aSentId": 20498, "answer": "&gt; in relation to the Atari paper and partly on your statement about it\n\nCan you point me to his statement about it?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20505, "question": "Two questions, if I may:\n\n1. With Moore's law gradually coming to an end, it would seem that we won't be achieving anything even close to General AI on today's hardware, at least not economically. As a researcher at the forefront of the field, are you aware of any hardware \"game changers\" that may simplify training and execution of extremely large neural networks that may be capable of intelligence?\n\n2. What are some of the most exciting papers that you have read (or written) in the past year?", "aSentId": 20506, "answer": "&gt; With Moore's law gradually coming to an end\n\nSource? GPUs have just picked up the Moore torch and is now carrying the field. Ive seen no reason why this won't continue for 1 or 2 more cycles before something new  like graphene will be in production.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20509, "question": "What advice do you have for a BTech computer science student passionate about strong AI hoping to join your team at IDSIA someday?", "aSentId": 20510, "answer": "Read our papers, re-implement one of our systems, perhaps improve it a bit, or better a lot, or do something else that I was not able to think of because it\u2019s too original!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20515, "question": "Where did you get the joke about the three prisoners? ", "aSentId": 20516, "answer": "You mean the one that starts: \"Three prisoners walk into a bar ...\"? :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20522, "question": "What is the algorithm of love?\n", "aSentId": 20523, "answer": "For those who did not grok: Schmidhuber works on the formal theory of curiosity and epistemic value. What is the best formal account of co-operation / affection / attachment, a.k.a. \"love\"? For instance, Minsky refers to \"attachment learning\", albeit without formalization.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20522, "question": "What is the algorithm of love?\n", "aSentId": 20525, "answer": "Great question!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20522, "question": "What is the algorithm of love?\n", "aSentId": 20527, "answer": "In response to the foolish comment: I am not a chinaman, but you are a racist village idiot.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20540, "question": "Do you think having a PhD is important if one wants to work in a good research team?", "aSentId": 20541, "answer": "Not at all - my PhD students are doing excellent work, but don't have a PhD :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20543, "question": "How feasible is it for a non-expert to successfully run RNN code on a new dataset? Is there any high-quality open source code to do it?", "aSentId": 20544, "answer": "alex graves has a toolbox called RNNLIB. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20546, "question": "i understand that neural networks and deep learning are computationally intensive for non-trivial problems. In addition, many experiments are necessary to see what works and what does not. What sort of equipment do you recommend for doing research in this area without breaking the bank? ", "aSentId": 20547, "answer": "As long as your applications are not too ambitious, a desktop machine with one or more GPUs should do!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20556, "question": "What do you think of Bitcoin. ", "aSentId": 20557, "answer": "I thought more of it when I had more of it.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20559, "question": "Why do so many chinamen flood the ML community with rubbish?", "aSentId": 20560, "answer": "Whoops, looks like Grandma found Reddit", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20563, "question": "Machine Intelligence Company Infographic (One Page)", "aSentId": 20564, "answer": "As one of the co-founders of Sight Machine I appreciate you including us in the the list. :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20563, "question": "Machine Intelligence Company Infographic (One Page)", "aSentId": 20566, "answer": "Very interesting - thank you for sharing!!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20563, "question": "Machine Intelligence Company Infographic (One Page)", "aSentId": 20568, "answer": "This was an excellent read and had a great graphic. Thank you. \n\nI need a left brain to go with my right brain!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20570, "question": "It looks like, she forgot about my megaproject:\n\nhttps://github.com/Eug145/TetrAI/blob/master/Source-v0.92beta/aimodule_a.cpp#L231\n\nOk... I do not need those hundreds of millions of USD, like Hassabis and others do... Actually, 7-8 will be enough for me to finish that work, I think.", "aSentId": 20571, "answer": "No idea why you're being downvoted. Sorry.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20570, "question": "It looks like, she forgot about my megaproject:\n\nhttps://github.com/Eug145/TetrAI/blob/master/Source-v0.92beta/aimodule_a.cpp#L231\n\nOk... I do not need those hundreds of millions of USD, like Hassabis and others do... Actually, 7-8 will be enough for me to finish that work, I think.", "aSentId": 20573, "answer": "To those, who ferociously downvoting my messages. Seriously, think about it. Have you ever seen something similar to my project? And if you have not, isn't it worth to be finished just to check that hypothesis? It is not about millions of USD, as you can see.\n\nAny thoughts or nope? ...Ok (:", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20574, "question": "To those, who ferociously downvoting my messages. Seriously, think about it. Have you ever seen something similar to my project? And if you have not, isn't it worth to be finished just to check that hypothesis? It is not about millions of USD, as you can see.\n\nAny thoughts or nope? ...Ok (:", "aSentId": 20575, "answer": "I've just so happen to have a few million lying around I'd like to invest.  Can I see your business plan?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20576, "question": "I've just so happen to have a few million lying around I'd like to invest.  Can I see your business plan?", "aSentId": 20577, "answer": "I am going to sleep, there is 1:23 AM in Ukraine now. Then I wake up,  you bring me O-1B visa, I travel to the USA and help your company invest your millions right... PROFIT?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20579, "question": "Introducing Sense - A Next-Generation Platform for Data Science", "aSentId": 20580, "answer": "Hi All -  Sense cofounder here.   We just opened Sense to the public.  Happy to answer any questions.  Hope you like it!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20581, "question": "Hi All -  Sense cofounder here.   We just opened Sense to the public.  Happy to answer any questions.  Hope you like it!", "aSentId": 20582, "answer": "This is pretty awesome, imo. Nicely done.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20581, "question": "Hi All -  Sense cofounder here.   We just opened Sense to the public.  Happy to answer any questions.  Hope you like it!", "aSentId": 20584, "answer": "Looks fantastic. \n\nAnyway:\n\n1. I did not find any info on the pricing model; yet that is extremly relevant. What do you charge?\n2. Any plans to add GPU nodes?\n3. I will not leave vi for my main coding. I will not use your online editor for more than tiny bits. Just being honest here. It would be great if I could push/pull to the code via git. Did not find it, although you can start projects from git somehow. Plans to add that?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20585, "question": "Looks fantastic. \n\nAnyway:\n\n1. I did not find any info on the pricing model; yet that is extremly relevant. What do you charge?\n2. Any plans to add GPU nodes?\n3. I will not leave vi for my main coding. I will not use your online editor for more than tiny bits. Just being honest here. It would be great if I could push/pull to the code via git. Did not find it, although you can start projects from git somehow. Plans to add that?", "aSentId": 20586, "answer": "Thanks.\n\n1. The pricing model for personal projects is currently usage only.   See the pricing page.\n2. We do have plans for GPU support and can offer this with Sense Enterprise already.\n3. We are aware of this concern.  There are advantages to an integrated IDE for data science, but we have ideas to ease these workflows for different types of users.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20579, "question": "Introducing Sense - A Next-Generation Platform for Data Science", "aSentId": 20588, "answer": "Can this be used for image recognition/ object detection?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20590, "question": "Will Copying the weights of a shallow Neural Networks to a Deep architecture help with performance?", "aSentId": 20591, "answer": "In some recent models, a backpropagation is started in many points of the neural network to force lower layers to produce discriminant features earlier during training (Inception model).\n\nYou can also initialize your model layer by layer with a generative pre-training.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20590, "question": "Will Copying the weights of a shallow Neural Networks to a Deep architecture help with performance?", "aSentId": 20593, "answer": "Yea, some deeper conv nets start by training an 8 layer version, and then moving on to the deeper version with those pretrained weights. Alternatively, you can make sure you have a good weight initialization, or just use batch normalization which seems to overcome this issue.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20590, "question": "Will Copying the weights of a shallow Neural Networks to a Deep architecture help with performance?", "aSentId": 20595, "answer": "QUOTE: *\"we trained networks with 1000 layers\"*. [Source](http://arxiv.org/abs/1412.6558).\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20596, "question": "QUOTE: *\"we trained networks with 1000 layers\"*. [Source](http://arxiv.org/abs/1412.6558).\n\n", "aSentId": 20597, "answer": "training a network to overfit is not so difficult. It's unclear if those results show anything useful.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20598, "question": "training a network to overfit is not so difficult. It's unclear if those results show anything useful.", "aSentId": 20599, "answer": "&gt; as the numbers of layers increases the back propagation algorithm struggles to affect the weights in the first few hidden layers\n\nIt shows that above statement from OP is not true anymore.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20601, "question": "From Maxout to Channel-Out: Encoding Information on Sparse Pathways", "aSentId": 20602, "answer": "Do I understand it correctly that they just ignored unabeled data in STL-10?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20601, "question": "From Maxout to Channel-Out: Encoding Information on Sparse Pathways", "aSentId": 20604, "answer": "I believe that this is the same as LWTA.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20606, "question": "Suggestions for a problem statement surrounding mining of Twitter conversation threads.", "aSentId": 20607, "answer": "You are aware of this work made around Twitter conversations? http://arxiv.org/abs/1503.02364\n\nTheir dataset awailable I believe. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20610, "question": "Calculating the gradient of Batch/Stochastic gradient descent", "aSentId": 20611, "answer": "Here is your mistake: \n&gt;derivative of data with respect to w.\n\nCalculate the derivative of the error (calculated in step 2) wrt w.  Basic chain rule.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20612, "question": "Here is your mistake: \n&gt;derivative of data with respect to w.\n\nCalculate the derivative of the error (calculated in step 2) wrt w.  Basic chain rule.", "aSentId": 20613, "answer": "Let error be a vector the same size as t (ex. error = [1,0]) and weight vector be it's own size (ex. w = [.5, 1, .7])\n\nWhat does it mean to derive error wrt w?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20614, "question": "Let error be a vector the same size as t (ex. error = [1,0]) and weight vector be it's own size (ex. w = [.5, 1, .7])\n\nWhat does it mean to derive error wrt w?", "aSentId": 20615, "answer": "Close. Error is a scalar for each training example. Typically error for a batch (or minibatch) is the average of the errors over each example in the batch. Compute the gradient of that.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20617, "question": "I have a vector of data on national capabilities for states. What ML algorithim could I use to reasonably divide this data up into three groups?", "aSentId": 20618, "answer": "The most obvious thing to try is k-means with k set to 3. See how well that works. \n\nYou'd have to make sure you have a reasonable distance measure between states, but without knowing what kind of data you have I can't say more...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20619, "question": "The most obvious thing to try is k-means with k set to 3. See how well that works. \n\nYou'd have to make sure you have a reasonable distance measure between states, but without knowing what kind of data you have I can't say more...", "aSentId": 20620, "answer": "Thanks. Its cinc scores from the correlates of war dataset.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20621, "question": "Thanks. Its cinc scores from the correlates of war dataset.", "aSentId": 20622, "answer": "So it's a 1-dimensional dataset? I guess you could still try k-means (and use Euclidean distance), but there might be better methods.\n\nAlso if you have the individual ratios it might make sense to cluster them in that space instead of using their averages.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20617, "question": "I have a vector of data on national capabilities for states. What ML algorithim could I use to reasonably divide this data up into three groups?", "aSentId": 20624, "answer": "Give me sample data (About 30 samples per category).  \n  \nI will write up an implementation for you.  I would side with LDA. But Im not sure what state powers means or how any of that is quantified. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20626, "question": "Generating 3D scenes from text [PDF]", "aSentId": 20627, "answer": "Very, very nice.\n\nI'm surprised they didn't chain this together (yet) with the research by Karpathy and Fei-Fei to have an image recognition system that can train itself.\n\nI would also like to see these 3D scenes tied to inner neurons in a conv net to generate even more convolutions.\n\nBut this is excellent research... this type of researches actually significantly helps speech recognition, because the computer can quickly learn when a predicted sentence doesn't make sense, or is a nonsense sentence. Which also is a step to visually creative algorithms.\n\nAnother next step is adding a physics simulation engine, so the 3D scenes are more than static representations-- the machines can then logically predict the future.\n\nAnd when these simulations become more complex, the computers will begin to find novel physical properties of the universe-- they'll be able to imagine what riding on photon is like.\n\nVery exciting times.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20629, "question": "DIGITS: Deep Learning GPU Training System", "aSentId": 20630, "answer": "Nvidia really jumped in with both feet today as far as ML goes. The Titan X looks like a beast of a card, and is designed for float32, combined with the 12 gb of ram I'm pretty sure I'll be picking one up here soon.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20631, "question": "Nvidia really jumped in with both feet today as far as ML goes. The Titan X looks like a beast of a card, and is designed for float32, combined with the 12 gb of ram I'm pretty sure I'll be picking one up here soon.", "aSentId": 20632, "answer": "We just picked a K80 for work. Can't wait to try it out, 24gb is insane.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20633, "question": "We just picked a K80 for work. Can't wait to try it out, 24gb is insane.", "aSentId": 20634, "answer": "I heard it's defective you should probably send it to me for testing your welcome  \n\n*cries in corner with K20*", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20633, "question": "We just picked a K80 for work. Can't wait to try it out, 24gb is insane.", "aSentId": 20636, "answer": "Sooo incredibly jealous. Enjoy! ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20633, "question": "We just picked a K80 for work. Can't wait to try it out, 24gb is insane.", "aSentId": 20638, "answer": "24gb, so hype.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20629, "question": "DIGITS: Deep Learning GPU Training System", "aSentId": 20640, "answer": "Wow this is really cool!\n\nBet it'll work on my Gtx 560-448 though :-/\n\nor maybe it does, anyone knows, can't seem to find any specs about hardware?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20629, "question": "DIGITS: Deep Learning GPU Training System", "aSentId": 20642, "answer": "Hi, readers i have this interesting problem, one of the FabMaker in Philippines is looking for software developers to develop a software to model the amputee limbs so that he can 3d Print it for poor and needy. He has already made a 3D scanner using Kinnect, but i was wondering if we can use Machine Learning to solve this modeling issue. We can take millions of reading of Limbs (both Right and Left) and them may be Machine can learn from it and given a set of parameter's it can predict the dimensions of other limb. So is this a valid use of machine learning or i am missing something ? Edit1: Sorry guys wrong Sub reddit, pardon my haste and ignorance.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20643, "question": "Hi, readers i have this interesting problem, one of the FabMaker in Philippines is looking for software developers to develop a software to model the amputee limbs so that he can 3d Print it for poor and needy. He has already made a 3D scanner using Kinnect, but i was wondering if we can use Machine Learning to solve this modeling issue. We can take millions of reading of Limbs (both Right and Left) and them may be Machine can learn from it and given a set of parameter's it can predict the dimensions of other limb. So is this a valid use of machine learning or i am missing something ? Edit1: Sorry guys wrong Sub reddit, pardon my haste and ignorance.", "aSentId": 20644, "answer": "Valid question, wrong place to ask. I'd start my own thread if I were you.. and try to be as explicit as possible. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20643, "question": "Hi, readers i have this interesting problem, one of the FabMaker in Philippines is looking for software developers to develop a software to model the amputee limbs so that he can 3d Print it for poor and needy. He has already made a 3D scanner using Kinnect, but i was wondering if we can use Machine Learning to solve this modeling issue. We can take millions of reading of Limbs (both Right and Left) and them may be Machine can learn from it and given a set of parameter's it can predict the dimensions of other limb. So is this a valid use of machine learning or i am missing something ? Edit1: Sorry guys wrong Sub reddit, pardon my haste and ignorance.", "aSentId": 20646, "answer": "That's not exactly what they do here but perhaps I can help. I'm not familiar with modeling but I am with all the rest. Machine learning would be used in this application if you had some data representing the same object. Taking millions of scans of that leg won't get you closer to modeling the leg in 3d. You want to use a distance measuring device to capture the dimensions of the leg. This gives you what's called a point cloud representation. From there you can apply some kind of *smooth fitting manifold* to create a surface. If you were to use machine learning it would be to compute the parameters of the manifold but a good mathematical (probably spline) transformation should do just fine. At any rate the more datapoints the better. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20648, "question": "Conformal Prediction in Python", "aSentId": 20649, "answer": "I like. Some introductory docs would be welcome.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20650, "question": "I like. Some introductory docs would be welcome.", "aSentId": 20651, "answer": "Thanks! There's a bunch of stuff I want to implement still, but providing documentation (with references) for the classification modules is at the top of my todo-list.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20648, "question": "Conformal Prediction in Python", "aSentId": 20653, "answer": "Neat! I'll have to play with it when I'm at work tomorrow.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20654, "question": "Neat! I'll have to play with it when I'm at work tomorrow.", "aSentId": 20655, "answer": "Thanks. Let me know what you think! :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20657, "question": "Netflix' RAD and PCA on 1D time series", "aSentId": 20658, "answer": "From what I remember of the paper, you fix some assumed periodicity in your data and then chop your data into segments of that length and use each period as a single column of your matrix.  So if you have hourly sampled data, you could put each (e.g.) 24 hours on a different column.  RPCA then recovers what amounts to a mixture across some prototypical periods, with sparse but unbounded 'anomalies' in the residual matrix.\n\nIf you don't know the periodicity of your system, I guess you could use something like a chi-squared periodogram to estimate it first, perhaps swapping the chi-squared statistic for something a bit more robust to outliers to avoid it trying to spuriously align on those.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20659, "question": "From what I remember of the paper, you fix some assumed periodicity in your data and then chop your data into segments of that length and use each period as a single column of your matrix.  So if you have hourly sampled data, you could put each (e.g.) 24 hours on a different column.  RPCA then recovers what amounts to a mixture across some prototypical periods, with sparse but unbounded 'anomalies' in the residual matrix.\n\nIf you don't know the periodicity of your system, I guess you could use something like a chi-squared periodogram to estimate it first, perhaps swapping the chi-squared statistic for something a bit more robust to outliers to avoid it trying to spuriously align on those.", "aSentId": 20660, "answer": "This is how they did it - you are spot on. You can also just do it at multiple scales - hourly, day/night, daily, weekly, monthly, seasonal, etc. These will give different anomalies, but you then need some follow-on decision making to decide which anomalies you actually care about.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20662, "question": "Machine Learning : Can it Help in this case ?", "aSentId": 20663, "answer": "Part of the reason why prosthetics are usually so expensive is the large amount of custom design that goes into each one. Every amputation is going to be different, making any sort of general predictions about what a prosthetic limb should look like very difficult. I'm not sure if machine learning is a good fit for this problem.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20664, "question": "Part of the reason why prosthetics are usually so expensive is the large amount of custom design that goes into each one. Every amputation is going to be different, making any sort of general predictions about what a prosthetic limb should look like very difficult. I'm not sure if machine learning is a good fit for this problem.", "aSentId": 20665, "answer": "Yes, but we want to help a lot of poor and needy where even the ability to walk will help alleviate a lot of their day to day problems. So any help is a big help in this part of the world.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20662, "question": "Machine Learning : Can it Help in this case ?", "aSentId": 20667, "answer": "I'm not clear on what your predictors are? Do you mean, for example, you would provide your algorithm with the dimensions of a right limb and predict the dimensions of the left? In that case wouldn't just mirroring the limb be easier?\n\nOr do you mean you will give measurements from other parts of the body to determine the limb size? Because that sounds doable, as long as your measurements are strong enough predictors. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20668, "question": "I'm not clear on what your predictors are? Do you mean, for example, you would provide your algorithm with the dimensions of a right limb and predict the dimensions of the left? In that case wouldn't just mirroring the limb be easier?\n\nOr do you mean you will give measurements from other parts of the body to determine the limb size? Because that sounds doable, as long as your measurements are strong enough predictors. ", "aSentId": 20669, "answer": "Your first assumption is correct, i plan to take lot of measurements of fully Abeled people with right and left leg and then given a data point of either right or left leg machine should be able to predict the other one. Once the full model is developed a person can then tweak it as per the needs to customer, the idea being that the person making the data modeler need not do be forced to use complicated software's to design the leg and yet be able to design the drawing which is suitable for the patient at hand. Measurements can be bone lengths of leg , width, height of person, his weight etc.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20670, "question": "Your first assumption is correct, i plan to take lot of measurements of fully Abeled people with right and left leg and then given a data point of either right or left leg machine should be able to predict the other one. Once the full model is developed a person can then tweak it as per the needs to customer, the idea being that the person making the data modeler need not do be forced to use complicated software's to design the leg and yet be able to design the drawing which is suitable for the patient at hand. Measurements can be bone lengths of leg , width, height of person, his weight etc.", "aSentId": 20671, "answer": "I've got to be missing something here. I can build a model right now that predicts the length of a right leg given a left leg.  Watch this:\nX = Y", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20662, "question": "Machine Learning : Can it Help in this case ?", "aSentId": 20673, "answer": "It depends on how much data you have since all people are different. I would say that regression could provide an approximate solution. The problem would be, as you implied, in overfitting the model, but there are ways to alleviate this.\n\nAlso, since I know nothing of prostethics - would it help to have a rough model and then fine tune it rather than do all work from scratch every time? Lets say you know that with this type of amputation, and this person heigh and weight, and this much of a limb cut off, and whatever other anthromopetric measures are, you need prosthetic type A otherwise you need prosthetic type B (I am of course greatly simplifying things). Then you could use classification techniques, e.g. you could try to select appropriate prosthetic and then modify it manually.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20677, "question": "Deep Representation Learning with Target Coding -- achieves 11% accuracy improvement, compared to 1-of-K, on MNIST, STL-10, CIFAR-100 (PDF)", "aSentId": 20678, "answer": "interesting, but boy are they cherry-picking their comparisions.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20681, "question": "pseudocode of gradient descent", "aSentId": 20682, "answer": "Pseudocode of gradient descent:\n\n    x,y &lt;- next training data point\n    y_hat = model(x; weights)  // evaluate your model (with weights =weights) on x to get a prediction\n    this_loss = loss(y_hat, y) // evaluate the loss\n    grad_loss = d/dy_hat loss(y_hat, y) // evaluate the gradient of the loss with respect to the prediction\n    grad_weights = grad_loss * d/dweights model(x; weights) // and now the gradient with respect to the weights\n    weights &lt;- weights - step_size * grad_weights // move weights towards lower loss", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20683, "question": "Pseudocode of gradient descent:\n\n    x,y &lt;- next training data point\n    y_hat = model(x; weights)  // evaluate your model (with weights =weights) on x to get a prediction\n    this_loss = loss(y_hat, y) // evaluate the loss\n    grad_loss = d/dy_hat loss(y_hat, y) // evaluate the gradient of the loss with respect to the prediction\n    grad_weights = grad_loss * d/dweights model(x; weights) // and now the gradient with respect to the weights\n    weights &lt;- weights - step_size * grad_weights // move weights towards lower loss", "aSentId": 20684, "answer": "Just for clarification, this would the the *online* GD (i.e., one training point at a time). There is also the batch version, which computes and adds-up all the gradients (for each point in the training dataset) for a given set of weights, before updating the weights.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20681, "question": "pseudocode of gradient descent", "aSentId": 20686, "answer": "you again. fuck off.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20688, "question": "Let\u2019s build open source tensor libraries for data science", "aSentId": 20689, "answer": "A C library from Ronan Collobert/the Torch folks: https://github.com/torch/TH this should be fairly easy to wrap from other languages.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20688, "question": "Let\u2019s build open source tensor libraries for data science", "aSentId": 20691, "answer": "I have been looking forward to testing this out:\n\nhttps://github.com/deeplearning4j/nd4j", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20693, "question": "Classifying plankton with deep neural networks", "aSentId": 20694, "answer": "Finally, a half-decent way to classify plankton.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20695, "question": "Finally, a half-decent way to classify plankton.", "aSentId": 20696, "answer": "Wondering, people upvote post above because it is sarcastic? Or it is real problem and big part of community is involved in plankton classification or at least sympathetic to those who classify plankton? Just curious.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20697, "question": "Wondering, people upvote post above because it is sarcastic? Or it is real problem and big part of community is involved in plankton classification or at least sympathetic to those who classify plankton? Just curious.", "aSentId": 20698, "answer": "It was just a joke is all. Classifying plankton sounds like such an incredibly specific thing. I didn't mean anything negative by it. It was actually an enlightening post.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20695, "question": "Finally, a half-decent way to classify plankton.", "aSentId": 20700, "answer": "You just made me nearly cry with laughter. I wish I were exaggerating - this was just the most unexpected sarcastic comment I've read on reddit in weeks. Thank you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20693, "question": "Classifying plankton with deep neural networks", "aSentId": 20702, "answer": "fantastic post, thank you!  reminds me how far I have to go.\n\nhow many GPUs do you have in the lab?   ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20703, "question": "fantastic post, thank you!  reminds me how far I have to go.\n\nhow many GPUs do you have in the lab?   ", "aSentId": 20704, "answer": "Thanks! Right now we have 9 that are fully functional and 2 that sort of work intermittently, but are unfortunately unusable for long-running experiments. Sharing these among 7 people was a bit of a challenge (normally they are not so heavily used) but it worked out well in the end. I realize that we are pretty spoiled in this regard!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20705, "question": "Thanks! Right now we have 9 that are fully functional and 2 that sort of work intermittently, but are unfortunately unusable for long-running experiments. Sharing these among 7 people was a bit of a challenge (normally they are not so heavily used) but it worked out well in the end. I realize that we are pretty spoiled in this regard!", "aSentId": 20706, "answer": "so everyone was individually running experiments on their own GPU? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20707, "question": "so everyone was individually running experiments on their own GPU? ", "aSentId": 20708, "answer": "pretty much, occasionally we had a few more available when other people weren't using any :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20709, "question": "pretty much, occasionally we had a few more available when other people weren't using any :)", "aSentId": 20710, "answer": "ah, interesting.  might be worth updating the post to talk about your hardware.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20693, "question": "Classifying plankton with deep neural networks", "aSentId": 20712, "answer": "Looking forward to the code dump!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20715, "question": "Kaggle gets a bit of heat around here for not being super \"realistic\", but I gotta say it's great to see competition forcing people to try very very recent developments like PReLu, batch-normalization, ADAM, orthogonal initialization, etc. Vetting these techniques is great for pushing the field forward. Awesome post.", "aSentId": 20716, "answer": "I feel once the prize money is over 10K and the dataset is of decent quality, you can get very interesting/relevant results out of a Kaggle competition.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20718, "question": "I wish they would open their training log so we can tell what affected how.\n\nAlthough this post is very helpful and worth more than the prize money.", "aSentId": 20719, "answer": "If you're talking about the spreadsheet with our results, that's pretty cryptic unfortunately! We came up with a lot of jargon to describe what we were doing to each other ('TTA' is one example). But if you have any questions about particular techniques I can do my best to answer them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20720, "question": "If you're talking about the spreadsheet with our results, that's pretty cryptic unfortunately! We came up with a lot of jargon to describe what we were doing to each other ('TTA' is one example). But if you have any questions about particular techniques I can do my best to answer them.", "aSentId": 20721, "answer": "TTA is not cryptic - anyone who would go to the length of taking a look at internal spreadsheet doc have read the blog post and know that it is Test Time Augmentation. Which is a clever idea by itself:)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20722, "question": "TTA is not cryptic - anyone who would go to the length of taking a look at internal spreadsheet doc have read the blog post and know that it is Test Time Augmentation. Which is a clever idea by itself:)", "aSentId": 20723, "answer": "That was just an example, our models received names like 'c6n2d3_fit_512m2_rn64' and 'cds_5pool3s2_22333_lkr_utb_gd3' :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20724, "question": "That was just an example, our models received names like 'c6n2d3_fit_512m2_rn64' and 'cds_5pool3s2_22333_lkr_utb_gd3' :)", "aSentId": 20725, "answer": "I'm glad you developed a useful hash for their names! ;)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20724, "question": "That was just an example, our models received names like 'c6n2d3_fit_512m2_rn64' and 'cds_5pool3s2_22333_lkr_utb_gd3' :)", "aSentId": 20727, "answer": "excuses! :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20729, "question": "I was just looking for a discussion of practical considerations for training deep convnets, and just I started using Lasagne (due to the excellent article http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/), so this is perfect! Thanks, can't wait to see the code! Also looking forward to batch normalization support in Lasagne; from what people are saying it sounds like it could be about as useful as dropout.", "aSentId": 20730, "answer": "The article states that batch normalization did not work out for them:\n\n&gt; batch normalization: unfortunately we were unable to reproduce the spectacular improvements in convergence speed described by Ioffe and Szegedy for our models.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20731, "question": "The article states that batch normalization did not work out for them:\n\n&gt; batch normalization: unfortunately we were unable to reproduce the spectacular improvements in convergence speed described by Ioffe and Szegedy for our models.", "aSentId": 20732, "answer": "We're not sure why this is - possibly because our use of orthogonal initialization already makes the network a lot easier to train, but it could also be a problem-specific thing. Or we had a bug in our code :) I definitely want to experiment further with it at some point.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20733, "question": "We're not sure why this is - possibly because our use of orthogonal initialization already makes the network a lot easier to train, but it could also be a problem-specific thing. Or we had a bug in our code :) I definitely want to experiment further with it at some point.", "aSentId": 20734, "answer": "&gt;orthogonal initialization already makes the network a lot easier to train  \n\nI'm no wizard but I really suspect it's this. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20735, "question": "&gt;orthogonal initialization already makes the network a lot easier to train  \n\nI'm no wizard but I really suspect it's this. ", "aSentId": 20736, "answer": "It's plausible. In the BN paper they do state that they initialize the weights to \"small random Gaussian values\".\n\nI suspect that BN would still have an advantage with orthogonal initialization, because it reduces covariate shift continuously during training, even if the gains would not be as spectacular.\n\nIt would be great if someone tried to reproduce their MNIST experiments, and also gave orthogonal initialization a go for comparison purposes.\n\nI would do it and write a blog post about it, if it weren't for the fact that I need to finish my dissertation ASAP :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20739, "question": "Congrats on the win! Are the prize money staying in the lab or going to the researchers?", "aSentId": 20740, "answer": "Probably the latter. Not entirely sure yet what will happen!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20743, "question": "Congrats to you and your teammates! Something this clearly shows, is how the success of deep learning (models) involves a lot of engineering, computing power and tweaking, which sometimes are left out of papers.", "aSentId": 20744, "answer": "I've found his blog posts about 20x as useful as current state of the art papers tbh. The galaxy classification writeup and code got me into deep learning.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20745, "question": "I've found his blog posts about 20x as useful as current state of the art papers tbh. The galaxy classification writeup and code got me into deep learning.", "aSentId": 20746, "answer": "That is really awesome to read :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20747, "question": "That is really awesome to read :)", "aSentId": 20748, "answer": "Yeah makes me wish I could have started my now defunct PhD over again.  \n\nOh well, I've managed to apply it at work once so far; hopefully many more to come!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20752, "question": "apsis - Bayesian Hyperparameter Optimization", "aSentId": 20753, "answer": "At a quick glance through the paper and docs, I didn't see a comparison to other Bayesian optimization packages or a story for why one should use this rather than, say, spearmint. Are there algorithmic improvements? A different API? More permissive license?\n\nObviously you do have such a story or you wouldn't have bothered writing this package. I think you should make that prominent -- put it in the README file, abstract of the paper, first section of the docs, etc. A discussion of where the comparative advantage lies versus other tools would go a long way towards helping the people who ought to be using this package realize that they ought to be using it. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20754, "question": "At a quick glance through the paper and docs, I didn't see a comparison to other Bayesian optimization packages or a story for why one should use this rather than, say, spearmint. Are there algorithmic improvements? A different API? More permissive license?\n\nObviously you do have such a story or you wouldn't have bothered writing this package. I think you should make that prominent -- put it in the README file, abstract of the paper, first section of the docs, etc. A discussion of where the comparative advantage lies versus other tools would go a long way towards helping the people who ought to be using this package realize that they ought to be using it. ", "aSentId": 20755, "answer": "You are correct, of course - so let me try adding this here.\n\n\nThere are currently - to my knowledge - five different packages for hyperparameter optimization. Let me try listing the main advantages.\n\n\n    whetlab: They're a company, we aren't.\n\n    spearmint: License (we're under MIT) and hopefully ease of use. Also, most of current their work seems concentrated on whetlab.\n\n    hyperopt: Different algorithm (TPE). Last commit has been half a year ago.\n\n    SMAC: Easier usage, I believe. Also, license again.\n\n    MOE: I have never used it, since it's been released after we began developement. It looks similar, though.\n\nWithout using a comparison, our goal is to offer a package which you can run locally, can use for hyperparameter optimization whether on your own computer or a cluster (not yet implemented) and is easy to use. Also, to allow for the implementation of any algorithm. We currently offer BayOpt and RandomSearch, but the architecture is flexible enough to implement TPEs, for example.\n\n\nThere is actually a reason for why we didn't do much comparison. As I've written above, the package is currently usable for optimization on your local machine, but not yet in parallel. Therefore, we had to actually run these experiments on a local machine, with the corresponding time expenditures. We prioritized getting a real-world example - MNIST - instead of the other comparisons, mostly to show that it's actually useful.\n\n\nIn terms of performance, our current implementation is roughly at the state of 2012-2013, so I believe we wouldn't perform as good as most of the above. There are several algorithmic improvements possible - see the issue list to get a list of corresponding papers - and several parts of the architecture have been done with the aim of allowing us to implement them fairly easily. However, our current aim is to make optimization on clusters possible before that, since we believe it to be strongly needed.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20756, "question": "You are correct, of course - so let me try adding this here.\n\n\nThere are currently - to my knowledge - five different packages for hyperparameter optimization. Let me try listing the main advantages.\n\n\n    whetlab: They're a company, we aren't.\n\n    spearmint: License (we're under MIT) and hopefully ease of use. Also, most of current their work seems concentrated on whetlab.\n\n    hyperopt: Different algorithm (TPE). Last commit has been half a year ago.\n\n    SMAC: Easier usage, I believe. Also, license again.\n\n    MOE: I have never used it, since it's been released after we began developement. It looks similar, though.\n\nWithout using a comparison, our goal is to offer a package which you can run locally, can use for hyperparameter optimization whether on your own computer or a cluster (not yet implemented) and is easy to use. Also, to allow for the implementation of any algorithm. We currently offer BayOpt and RandomSearch, but the architecture is flexible enough to implement TPEs, for example.\n\n\nThere is actually a reason for why we didn't do much comparison. As I've written above, the package is currently usable for optimization on your local machine, but not yet in parallel. Therefore, we had to actually run these experiments on a local machine, with the corresponding time expenditures. We prioritized getting a real-world example - MNIST - instead of the other comparisons, mostly to show that it's actually useful.\n\n\nIn terms of performance, our current implementation is roughly at the state of 2012-2013, so I believe we wouldn't perform as good as most of the above. There are several algorithmic improvements possible - see the issue list to get a list of corresponding papers - and several parts of the architecture have been done with the aim of allowing us to implement them fairly easily. However, our current aim is to make optimization on clusters possible before that, since we believe it to be strongly needed.", "aSentId": 20757, "answer": "For me, a feature making the difference in hyperparam optimizers is conditionals. As in a HyperOpt example - a few classifiers, each with different set of params. Does apsis have this?\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20758, "question": "For me, a feature making the difference in hyperparam optimizers is conditionals. As in a HyperOpt example - a few classifiers, each with different set of params. Does apsis have this?\n", "aSentId": 20759, "answer": "Another example would be, I believe, optimizing a Neural Network's architecture. For each layer, you'd have the activation function, the number of neurons, maybe type of convolution (if CNN). If you don't have that layer, you don't need to optimize over the corresponding parameters - this, in fact, is just a useless distraction.\n\nThe short answer is no, apsis does not have this yet.\n\nThe long answer is that there is an issue for this ([#110](https://github.com/FrederikDiehl/apsis/issues/110)), and we did plan the architecture to allow us to support something like that. There is a paper talking about conditional parameter spaces in the context of Bayesian Optimization [1], so we do have a starting point for the implementation. This hasn't been done yet due to limited time, and as mentioned above, cluster support would be the step before any other extensions like that [2].\n\n\n[1] The Indiana Jones pun paper, http://arxiv.org/abs/1409.4011 (I've just updated the issue with the link, too.)\n\n[2] As always, of course, if you'd be like implementing something like that, feel free - we'd be very happy about any other contributors!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20756, "question": "You are correct, of course - so let me try adding this here.\n\n\nThere are currently - to my knowledge - five different packages for hyperparameter optimization. Let me try listing the main advantages.\n\n\n    whetlab: They're a company, we aren't.\n\n    spearmint: License (we're under MIT) and hopefully ease of use. Also, most of current their work seems concentrated on whetlab.\n\n    hyperopt: Different algorithm (TPE). Last commit has been half a year ago.\n\n    SMAC: Easier usage, I believe. Also, license again.\n\n    MOE: I have never used it, since it's been released after we began developement. It looks similar, though.\n\nWithout using a comparison, our goal is to offer a package which you can run locally, can use for hyperparameter optimization whether on your own computer or a cluster (not yet implemented) and is easy to use. Also, to allow for the implementation of any algorithm. We currently offer BayOpt and RandomSearch, but the architecture is flexible enough to implement TPEs, for example.\n\n\nThere is actually a reason for why we didn't do much comparison. As I've written above, the package is currently usable for optimization on your local machine, but not yet in parallel. Therefore, we had to actually run these experiments on a local machine, with the corresponding time expenditures. We prioritized getting a real-world example - MNIST - instead of the other comparisons, mostly to show that it's actually useful.\n\n\nIn terms of performance, our current implementation is roughly at the state of 2012-2013, so I believe we wouldn't perform as good as most of the above. There are several algorithmic improvements possible - see the issue list to get a list of corresponding papers - and several parts of the architecture have been done with the aim of allowing us to implement them fairly easily. However, our current aim is to make optimization on clusters possible before that, since we believe it to be strongly needed.", "aSentId": 20761, "answer": "Thanks! I'm only an interested bystander wrt bayesian optimization, so (even if it'd have been obvious after some research) this kind of survey of the current options is very useful.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20756, "question": "You are correct, of course - so let me try adding this here.\n\n\nThere are currently - to my knowledge - five different packages for hyperparameter optimization. Let me try listing the main advantages.\n\n\n    whetlab: They're a company, we aren't.\n\n    spearmint: License (we're under MIT) and hopefully ease of use. Also, most of current their work seems concentrated on whetlab.\n\n    hyperopt: Different algorithm (TPE). Last commit has been half a year ago.\n\n    SMAC: Easier usage, I believe. Also, license again.\n\n    MOE: I have never used it, since it's been released after we began developement. It looks similar, though.\n\nWithout using a comparison, our goal is to offer a package which you can run locally, can use for hyperparameter optimization whether on your own computer or a cluster (not yet implemented) and is easy to use. Also, to allow for the implementation of any algorithm. We currently offer BayOpt and RandomSearch, but the architecture is flexible enough to implement TPEs, for example.\n\n\nThere is actually a reason for why we didn't do much comparison. As I've written above, the package is currently usable for optimization on your local machine, but not yet in parallel. Therefore, we had to actually run these experiments on a local machine, with the corresponding time expenditures. We prioritized getting a real-world example - MNIST - instead of the other comparisons, mostly to show that it's actually useful.\n\n\nIn terms of performance, our current implementation is roughly at the state of 2012-2013, so I believe we wouldn't perform as good as most of the above. There are several algorithmic improvements possible - see the issue list to get a list of corresponding papers - and several parts of the architecture have been done with the aim of allowing us to implement them fairly easily. However, our current aim is to make optimization on clusters possible before that, since we believe it to be strongly needed.", "aSentId": 20763, "answer": "There is also https://sigopt.com/ which is the commercial version of MOE (much like how Whetlab is the commercial version of Spearmint).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20754, "question": "At a quick glance through the paper and docs, I didn't see a comparison to other Bayesian optimization packages or a story for why one should use this rather than, say, spearmint. Are there algorithmic improvements? A different API? More permissive license?\n\nObviously you do have such a story or you wouldn't have bothered writing this package. I think you should make that prominent -- put it in the README file, abstract of the paper, first section of the docs, etc. A discussion of where the comparative advantage lies versus other tools would go a long way towards helping the people who ought to be using this package realize that they ought to be using it. ", "aSentId": 20765, "answer": "&gt; Apsis is open-sourced under the MIT license.\n&gt; \n\nit's right there on the github README and the paper abstract.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20766, "question": "&gt; Apsis is open-sourced under the MIT license.\n&gt; \n\nit's right there on the github README and the paper abstract.\n\n", "aSentId": 20767, "answer": "That's why I mentioned licensing as a possibility -- but it's not clear whether the sole point of the project is \"spearmint with a more permissive license\" or if there are intended to be other advantages as well. \n\nIf the main advantage is licensing, it'd still help to describe explicitly some cases in which the spearmint GPL would be a problem. At first blush the GPL seems like it'd only matter if you wanted to modify and redistribute spearmint itself, not for the main use case of Bayesian optimization in which you train a model and then want to use/distribute the *model* for (potentially) commercial purposes. I admit I haven't thought very hard about this, but that probably makes me representative of a decent subset of potential users who *also* haven't necessarily thought hard about these issues. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20768, "question": "That's why I mentioned licensing as a possibility -- but it's not clear whether the sole point of the project is \"spearmint with a more permissive license\" or if there are intended to be other advantages as well. \n\nIf the main advantage is licensing, it'd still help to describe explicitly some cases in which the spearmint GPL would be a problem. At first blush the GPL seems like it'd only matter if you wanted to modify and redistribute spearmint itself, not for the main use case of Bayesian optimization in which you train a model and then want to use/distribute the *model* for (potentially) commercial purposes. I admit I haven't thought very hard about this, but that probably makes me representative of a decent subset of potential users who *also* haven't necessarily thought hard about these issues. ", "aSentId": 20769, "answer": "GPL spearmint is basically abandoned. the new project by Whetlab has a far more restrictive license.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20771, "question": "FastML: What you wanted to know about AI", "aSentId": 20772, "answer": "\"A bunch of bananas are hanging from a ceiling\"  \n\nMaybe they won't dominate us, but now they're also good at joking, damn!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20771, "question": "FastML: What you wanted to know about AI", "aSentId": 20774, "answer": "Seriously, I still don't understand what's so bad about super human A.I. But the field is huge and there is so much to explore, exactly the reason why I'm studying it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20771, "question": "FastML: What you wanted to know about AI", "aSentId": 20776, "answer": "I hope the author of this blog post understands that AI is one of those disciplines in which past progress is not really a predictor of that of the future.\n\nThe number of people who are after this problem, and the amount of money that is being poured into it, it's going to happen sooner than later. And it might very well be like a flick of a switch, instead of gradual progress towards better and better capabilities.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20777, "question": "I hope the author of this blog post understands that AI is one of those disciplines in which past progress is not really a predictor of that of the future.\n\nThe number of people who are after this problem, and the amount of money that is being poured into it, it's going to happen sooner than later. And it might very well be like a flick of a switch, instead of gradual progress towards better and better capabilities.", "aSentId": 20778, "answer": "In the 50s or 60s, shortly after the birth of the field, US military intended to create a computer able to automatically translate Russian. The timeframe they had: 2-3 years. And here we are, with automatic translation, half a century of pouring money later.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 20779, "question": "In the 50s or 60s, shortly after the birth of the field, US military intended to create a computer able to automatically translate Russian. The timeframe they had: 2-3 years. And here we are, with automatic translation, half a century of pouring money later.", "aSentId": 20780, "answer": "We got it 60 years later in pretty useable form, in 10 years it should be as good as human translation. It depended on a number of solutions to problems, but the biggest gain came with probabilistic models and hardware capable of running them on enough data.", "corpus": "reddit"}]
